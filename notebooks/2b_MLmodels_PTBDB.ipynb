{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ee4ebc0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-08T19:44:40.180264Z",
     "iopub.status.busy": "2024-03-08T19:44:40.179864Z",
     "iopub.status.idle": "2024-03-08T19:44:43.390911Z",
     "shell.execute_reply": "2024-03-08T19:44:43.389524Z"
    },
    "papermill": {
     "duration": 3.228191,
     "end_time": "2024-03-08T19:44:43.393819",
     "exception": false,
     "start_time": "2024-03-08T19:44:40.165628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pickle\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "import xgboost as XGB\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed72fbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/simon/.kaggle/kaggle.json'\n",
      "All Datasets are already available.\n"
     ]
    }
   ],
   "source": [
    "## Test cell for simon: Using Kaggle API to download the datasets indepent of github and its filesize limitations. Storing it in folder located outside of the repo.\n",
    "# If this works, all filepaths have to be adjusted in all notebooks to make use of the downloaded datasets.\n",
    "#RUN THIS CELL ONLY ONCE FOR ALL NOTEBOOKS!\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "#configuring and authentification with kaggle api. This could be configured so that a authentification mask is shown?\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "#Configuring the metadata for the ecg heartbeat data (original username etc)\n",
    "dataset_owner = \"shayanfazeli\"\n",
    "dataset_name = \"heartbeat\"\n",
    "\n",
    "#Configuring a download path that is NOT in the current github repo (so the big files are not pushed and cause an error!) --> Links to filepaths have to be dynamically adjusted\n",
    "download_path = \"../data/KAGGLE_datasets\" #In this case we use the data folder that is in the .gitignore list and therefore not pushed! To keep everything in one local repo.\n",
    "\n",
    "# Download structure: First check if dataset is already downloaded, else download it and store it in download path (should be outside git repo!)\n",
    "dataset_folder = os.path.join(download_path, dataset_name)\n",
    "if not os.path.exists(dataset_folder):\n",
    "    # Case 1: Dataset path is not created --> Create it and download datasets into it\n",
    "    api.dataset_download_files(dataset_owner + \"/\" + dataset_name, path=download_path + \"/\" + dataset_name, unzip=True)\n",
    "    print(\"Datasets are downloaded and unzipped.\")\n",
    "else:\n",
    "    # Case 2: Folder is created, but datasets might be missing\n",
    "    missing_files = [] \n",
    "    for file_name in [\"mitbih_test.csv\", \"mitbih_train.csv\", \"ptbdb_abnormal.csv\", \"ptbdb_normal.csv\"]:  # These are the hardcoded names of the datasets that should be downloaded.\n",
    "        file_path = os.path.join(dataset_folder, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            missing_files.append(file_name)\n",
    "\n",
    "    if missing_files:\n",
    "        # If the list contains missing files, download ALL files and overwrite the old folder.\n",
    "        api.dataset_download_files(dataset_owner + \"/\" + dataset_name, path=download_path + \"/\" + dataset_name, unzip=True, force=True)\n",
    "        print(\"Missing data was donwloaded and unzipped. All Datasets are now available.\")\n",
    "    else:\n",
    "        print(\"All Datasets are already available.\")\n",
    "\n",
    "#Creating new variable that links to the datasets and can be used in the rest of the code.\n",
    "path_to_datasets = download_path + \"/\" + dataset_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a63a020a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T19:44:43.419755Z",
     "iopub.status.busy": "2024-03-08T19:44:43.419203Z",
     "iopub.status.idle": "2024-03-08T19:44:43.424620Z",
     "shell.execute_reply": "2024-03-08T19:44:43.423364Z"
    },
    "papermill": {
     "duration": 0.021105,
     "end_time": "2024-03-08T19:44:43.426745",
     "exception": false,
     "start_time": "2024-03-08T19:44:43.405640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#only 4 digits are printed out by numpy calculations.\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb7976fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T19:44:43.452214Z",
     "iopub.status.busy": "2024-03-08T19:44:43.451260Z",
     "iopub.status.idle": "2024-03-08T19:44:45.018326Z",
     "shell.execute_reply": "2024-03-08T19:44:45.017357Z"
    },
    "papermill": {
     "duration": 1.582728,
     "end_time": "2024-03-08T19:44:45.021165",
     "exception": false,
     "start_time": "2024-03-08T19:44:43.438437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell now makes use of the downloadfolder for the datasets. If already available locally, the filepaths can be changed.\n",
    "ptbdb_normal= pd.read_csv(path_to_datasets + \"/\" +  'ptbdb_normal.csv', header=None)\n",
    "ptbdb_abnormal=pd.read_csv(path_to_datasets + \"/\" +   'ptbdb_abnormal.csv',header=None)\n",
    "\n",
    "#PTBDB comes preconfigures as normal and abnormal dataset, so we concate both, reshuffle them and then generate the test and train sets.\n",
    "ptbdb_combined = pd.concat([ptbdb_normal, ptbdb_abnormal], ignore_index=True, axis=0)\n",
    "\n",
    "#Reshuffle the whole new dataframe\n",
    "ptbdb_combined_shuffled = ptbdb_combined.sample(frac=1, random_state=42)\n",
    "\n",
    "#Generate Test and Train datasets\n",
    "X = ptbdb_combined_shuffled.iloc[:, :-1] #All values except the last column\n",
    "y = ptbdb_combined_shuffled.iloc[:, -1] #All values from the last column\n",
    "\n",
    "train, test, train_target, test_target = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b47f4078",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T19:44:45.046590Z",
     "iopub.status.busy": "2024-03-08T19:44:45.045632Z",
     "iopub.status.idle": "2024-03-08T19:44:45.050665Z",
     "shell.execute_reply": "2024-03-08T19:44:45.049737Z"
    },
    "papermill": {
     "duration": 0.019945,
     "end_time": "2024-03-08T19:44:45.052854",
     "exception": false,
     "start_time": "2024-03-08T19:44:45.032909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Switches for the user to define which sample method is used and which models are run.\n",
    "class Config:\n",
    "    oversample = False #refers to PTBDB B_SMOTE\n",
    "    undersample = False #Refers to undersampling with random undersampler\n",
    "    sample_name = \"UNDEFINED_SAMPLE\"\n",
    "\n",
    "Train_SVM =  True #trains the SVM Model without Gridsearch\n",
    "Train_KNN = True #trains the KNN Model without Gridsearch\n",
    "Train_DTC = True #trains the DTC Model without Gridsearch\n",
    "Train_RF = True #trains the RF Model without Gridsearch\n",
    "Train_XGB = True #trains the XGB Model without Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd5480da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T19:44:45.077988Z",
     "iopub.status.busy": "2024-03-08T19:44:45.076910Z",
     "iopub.status.idle": "2024-03-08T19:44:45.082093Z",
     "shell.execute_reply": "2024-03-08T19:44:45.081306Z"
    },
    "papermill": {
     "duration": 0.019846,
     "end_time": "2024-03-08T19:44:45.084238",
     "exception": false,
     "start_time": "2024-03-08T19:44:45.064392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "oversampler = SMOTE()\n",
    "undersampler = RandomUnderSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76f8e1da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T19:44:45.108902Z",
     "iopub.status.busy": "2024-03-08T19:44:45.108209Z",
     "iopub.status.idle": "2024-03-08T19:44:45.402040Z",
     "shell.execute_reply": "2024-03-08T19:44:45.400786Z"
    },
    "papermill": {
     "duration": 0.309274,
     "end_time": "2024-03-08T19:44:45.404886",
     "exception": false,
     "start_time": "2024-03-08T19:44:45.095612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the original PTBDB dataset\n",
      "Sample Name: PTBDB_A_Original\n"
     ]
    }
   ],
   "source": [
    "#Based on the user settings, Resampling is done and the sample name (i.e. filenames) are modified.\n",
    "if Config.oversample:\n",
    "    train, train_target = oversampler.fit_resample(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n",
    "    Config.sample_name = \"PTBDB_B_SMOTE\"\n",
    "    print(\"Sample Name:\", Config.sample_name)\n",
    "elif Config.undersample:\n",
    "    train, train_target = undersampler.fit_resample(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n",
    "    Config.sample_name = \"PTBDB_C_RUS\"\n",
    "    print(\"Sample Name:\", Config.sample_name)\n",
    "else: \n",
    "    print(\"Using the original PTBDB dataset\")\n",
    "    Config.sample_name = \"PTBDB_A_Original\"\n",
    "    print(\"Sample Name:\", Config.sample_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf34119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to save models and classification report directly after running.\n",
    "def save_model_and_report(model, report, model_filename, report_filename, model_folder=\"../models/ML_Models\", report_folder=\"../reports/figures/ML_Models\"):\n",
    "    # Save the model\n",
    "    model_savepath = os.path.join(model_folder, model_filename)\n",
    "    with open(model_savepath, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"The model was saved as {model_filename} in folder {model_folder}.\")\n",
    "\n",
    "    # Check if model file size is greater than 98MB (Restriction for github only!)\n",
    "    if os.path.getsize(model_savepath) > 98 * 1024 * 1024:  # Check if size is greater than 98MB\n",
    "        print(\"Model file size is too big. Changing save path...\")\n",
    "        model_folder = \"../data/models_too_big_for_git\"\n",
    "        os.makedirs(model_folder, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "        new_model_savepath = os.path.join(model_folder, model_filename)\n",
    "        os.replace(model_savepath, new_model_savepath)  # Move the model to the new location\n",
    "        print(f\"Model moved to {model_folder} due to its size.\")\n",
    "\n",
    "    # Save the classification report\n",
    "    report_savepath = os.path.join(report_folder, report_filename)\n",
    "    with open(report_savepath, \"w\") as f:\n",
    "        f.write(report)\n",
    "    print(f\"The classification report was saved as {report_filename} in folder {report_folder}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45018274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T19:44:45.429746Z",
     "iopub.status.busy": "2024-03-08T19:44:45.428967Z",
     "iopub.status.idle": "2024-03-08T19:44:45.437443Z",
     "shell.execute_reply": "2024-03-08T19:44:45.436329Z"
    },
    "papermill": {
     "duration": 0.023587,
     "end_time": "2024-03-08T19:44:45.439990",
     "exception": false,
     "start_time": "2024-03-08T19:44:45.416403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11641, 187)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcf7de9",
   "metadata": {
    "papermill": {
     "duration": 0.011083,
     "end_time": "2024-03-08T19:44:45.462683",
     "exception": false,
     "start_time": "2024-03-08T19:44:45.451600",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **SVM**\n",
    "\n",
    "<font color='red'>PUT SOME NOTES / EXPLANATORY WORDS AS TEAM HERE IF NEEDED</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "563b6a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8743    0.8037    0.8375       805\n",
      "         1.0     0.9272    0.9558    0.9413      2106\n",
      "\n",
      "    accuracy                         0.9138      2911\n",
      "   macro avg     0.9008    0.8798    0.8894      2911\n",
      "weighted avg     0.9126    0.9138    0.9126      2911\n",
      "\n",
      "The model was saved as SVM_Basemodel_no_gridsearch_PTBDB_A_Original.pkl in folder ../models/ML_Models.\n",
      "The classification report was saved as SVM_Basemodel_no_gridsearch_PTBDB_A_Original_classification_report.txt in folder ../reports/figures/ML_Models.\n"
     ]
    }
   ],
   "source": [
    "#Just the code for model creation, fitting and creating the report out of the predictions.\n",
    "if Train_SVM == True:\n",
    "    model = SVC(cache_size=500)\n",
    "    model.fit(train,train_target)\n",
    "    predictions = model.predict(test)\n",
    "    report=classification_report(test_target, predictions, digits=4)\n",
    "    print(report)\n",
    "    #Calling the save_model_and_report function for the SVM Model\n",
    "    model_filename = f\"SVM_Basemodel_no_gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"SVM_Basemodel_no_gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    save_model_and_report(model, report, model_filename, report_filename)\n",
    "else:\n",
    "    print(\"SVM Model withoud gridsearch is not trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f4bb81",
   "metadata": {},
   "source": [
    "# **KNN**\n",
    "\n",
    "<font color='red'>PUT SOME NOTES / EXPLANATORY WORDS AS TEAM HERE IF NEEDED</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ece6adb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8550    0.9081    0.8807       805\n",
      "         1.0     0.9640    0.9411    0.9524      2106\n",
      "\n",
      "    accuracy                         0.9320      2911\n",
      "   macro avg     0.9095    0.9246    0.9166      2911\n",
      "weighted avg     0.9339    0.9320    0.9326      2911\n",
      "\n",
      "The model was saved as KNN_Basemodel_no_gridsearch_PTBDB_A_Original.pkl in folder ../models/ML_Models.\n",
      "The classification report was saved as KNN_Basemodel_no_gridsearch_PTBDB_A_Original_classification_report.txt in folder ../reports/figures/ML_Models.\n"
     ]
    }
   ],
   "source": [
    "if Train_KNN == True:\n",
    "    model = KNN(n_jobs = -1)\n",
    "    model.fit(train,train_target)\n",
    "    predictions = model.predict(test)\n",
    "    report=classification_report(test_target, predictions, digits=4)\n",
    "    print(report)\n",
    "    #Calling the save_model_and_report function for the KNN Model\n",
    "    model_filename = f\"KNN_Basemodel_no_gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"KNN_Basemodel_no_gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    save_model_and_report(model, report, model_filename, report_filename)\n",
    "else:\n",
    "    print(\"KNN Model without gridsearch is not trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5682fbe",
   "metadata": {},
   "source": [
    "# **Decision Tree**\n",
    "\n",
    "<font color='red'>PUT SOME NOTES / EXPLANATORY WORDS AS TEAM HERE IF NEEDED</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3219cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8694    0.8683    0.8689       805\n",
      "         1.0     0.9497    0.9501    0.9499      2106\n",
      "\n",
      "    accuracy                         0.9275      2911\n",
      "   macro avg     0.9095    0.9092    0.9094      2911\n",
      "weighted avg     0.9275    0.9275    0.9275      2911\n",
      "\n",
      "The model was saved as DTC_Basemodel_no_gridsearch_PTBDB_A_Original.pkl in folder ../models/ML_Models.\n",
      "The classification report was saved as DTC_Basemodel_no_gridsearch_PTBDB_A_Original_classification_report.txt in folder ../reports/figures/ML_Models.\n"
     ]
    }
   ],
   "source": [
    "if Train_DTC == True:\n",
    "    model = DTC()\n",
    "    model.fit(train,train_target)\n",
    "    predictions = model.predict(test)\n",
    "    report=classification_report(test_target, predictions, digits=4)\n",
    "    print(report)\n",
    "    #Calling the save_model_and_report function for the Decision Tree Model\n",
    "    model_filename = f\"DTC_Basemodel_no_gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"DTC_Basemodel_no_gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    save_model_and_report(model, report, model_filename, report_filename)\n",
    "else:\n",
    "    print(\"Decision Tree Model without gridsearch is not trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4816091b",
   "metadata": {},
   "source": [
    "# **Random Forest**\n",
    "\n",
    "<font color='red'>PUT SOME NOTES / EXPLANATORY WORDS AS TEAM HERE IF NEEDED</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e063687d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9750    0.9217    0.9476       805\n",
      "         1.0     0.9707    0.9910    0.9807      2106\n",
      "\n",
      "    accuracy                         0.9718      2911\n",
      "   macro avg     0.9729    0.9564    0.9642      2911\n",
      "weighted avg     0.9719    0.9718    0.9716      2911\n",
      "\n",
      "The model was saved as RFC_Basemodel_no_gridsearch_PTBDB_A_Original.pkl in folder ../models/ML_Models.\n",
      "The classification report was saved as RFC_Basemodel_no_gridsearch_PTBDB_A_Original_classification_report.txt in folder ../reports/figures/ML_Models.\n"
     ]
    }
   ],
   "source": [
    "if Train_RF == True:\n",
    "    model = RFC(n_jobs = -1)\n",
    "    model.fit(train,train_target)\n",
    "    predictions = model.predict(test)\n",
    "    report=classification_report(test_target, predictions, digits=4)\n",
    "    print(report)\n",
    "    #Calling the save_model_and_report function for the Random Forest Model\n",
    "    model_filename = f\"RFC_Basemodel_no_gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"RFC_Basemodel_no_gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    save_model_and_report(model, report, model_filename, report_filename)\n",
    "else:\n",
    "    print(\"Random Forest Model without gridsearch is not trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18561269",
   "metadata": {},
   "source": [
    "# **XGBoost**\n",
    "\n",
    "<font color='red'>PUT SOME NOTES / EXPLANATORY WORDS AS TEAM HERE IF NEEDED</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed7698c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9710    0.9553    0.9631       805\n",
      "         1.0     0.9830    0.9891    0.9860      2106\n",
      "\n",
      "    accuracy                         0.9797      2911\n",
      "   macro avg     0.9770    0.9722    0.9745      2911\n",
      "weighted avg     0.9797    0.9797    0.9797      2911\n",
      "\n",
      "The model was saved as XGB_Basemodel_no_gridsearch_PTBDB_A_Original.pkl in folder ../models/ML_Models.\n",
      "The classification report was saved as XGB_Basemodel_no_gridsearch_PTBDB_A_Original_classification_report.txt in folder ../reports/figures/ML_Models.\n"
     ]
    }
   ],
   "source": [
    "if Train_XGB == True:\n",
    "    model = XGB.XGBClassifier(objective='binary:logistic')\n",
    "    model.fit(train,train_target)\n",
    "    predictions = model.predict(test)\n",
    "    report=classification_report(test_target, predictions, digits=4)\n",
    "    print(report)\n",
    "    #Calling the save_model_and_report function for the XGB Model\n",
    "    model_filename = f\"XGB_Basemodel_no_gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"XGB_Basemodel_no_gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    save_model_and_report(model, report, model_filename, report_filename)\n",
    "else:\n",
    "    print(\"XBG Model without gridsearch is not trained.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 29414,
     "sourceId": 37484,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 55.638252,
   "end_time": "2024-03-08T19:45:31.353184",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-08T19:44:35.714932",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
