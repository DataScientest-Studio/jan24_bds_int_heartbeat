{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-21T08:21:29.078990Z","iopub.status.busy":"2024-03-21T08:21:29.078141Z","iopub.status.idle":"2024-03-21T08:21:29.100221Z","shell.execute_reply":"2024-03-21T08:21:29.099372Z","shell.execute_reply.started":"2024-03-21T08:21:29.078956Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-26 09:56:59.668892: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-03-26 09:56:59.668959: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-03-26 09:56:59.719507: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-03-26 09:56:59.826115: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-03-26 09:57:00.993201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]},{"name":"stdout","output_type":"stream","text":["GPU is used\n"]},{"name":"stderr","output_type":"stream","text":["2024-03-26 09:57:02.292951: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2024-03-26 09:57:02.455888: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2024-03-26 09:57:02.455972: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n"]}],"source":["import numpy as np # linear algebra \n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from sklearn.model_selection import train_test_split\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","from sklearn.metrics import accuracy_score, classification_report\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","if tf.config.experimental.list_physical_devices('GPU'):\n","    print('GPU is used')\n","else:\n","    print('no GPU available, CPU is used instead')\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/simon/.kaggle/kaggle.json'\n","Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/simon/.kaggle/kaggle.json'\n","All Datasets are already available.\n"]}],"source":["## Test cell for simon: Using Kaggle API to download the datasets indepent of github and its filesize limitations. Storing it in folder located outside of the repo.\n","# If this works, all filepaths have to be adjusted in all notebooks to make use of the downloaded datasets.\n","#RUN THIS CELL ONLY ONCE FOR ALL NOTEBOOKS!\n","\n","from kaggle.api.kaggle_api_extended import KaggleApi\n","\n","#configuring and authentification with kaggle api. This could be configured so that a authentification mask is shown?\n","api = KaggleApi()\n","api.authenticate()\n","\n","#Configuring the metadata for the ecg heartbeat data (original username etc)\n","dataset_owner = \"shayanfazeli\"\n","dataset_name = \"heartbeat\"\n","\n","#Configuring a download path that is NOT in the current github repo (so the big files are not pushed and cause an error!) --> Links to filepaths have to be dynamically adjusted\n","download_path = \"../data/KAGGLE_datasets\" #In this case we use the data folder that is in the .gitignore list and therefore not pushed! To keep everything in one local repo.\n","\n","# Download structure: First check if dataset is already downloaded, else download it and store it in download path (should be outside git repo!)\n","dataset_folder = os.path.join(download_path, dataset_name)\n","if not os.path.exists(dataset_folder):\n","    # Case 1: Dataset path is not created --> Create it and download datasets into it\n","    api.dataset_download_files(dataset_owner + \"/\" + dataset_name, path=download_path + \"/\" + dataset_name, unzip=True)\n","    print(\"Datasets are downloaded and unzipped.\")\n","else:\n","    # Case 2: Folder is created, but datasets might be missing\n","    missing_files = [] \n","    for file_name in [\"mitbih_test.csv\", \"mitbih_train.csv\", \"ptbdb_abnormal.csv\", \"ptbdb_normal.csv\"]:  # These are the hardcoded names of the datasets that should be downloaded.\n","        file_path = os.path.join(dataset_folder, file_name)\n","        if not os.path.exists(file_path):\n","            missing_files.append(file_name)\n","\n","    if missing_files:\n","        # If the list contains missing files, download ALL files and overwrite the old folder.\n","        api.dataset_download_files(dataset_owner + \"/\" + dataset_name, path=download_path + \"/\" + dataset_name, unzip=True, force=True)\n","        print(\"Missing data was donwloaded and unzipped. All Datasets are now available.\")\n","    else:\n","        print(\"All Datasets are already available.\")\n","\n","#Creating new variable that links to the datasets and can be used in the rest of the code.\n","path_to_datasets = download_path + \"/\" + dataset_name "]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:29.102011Z","iopub.status.busy":"2024-03-21T08:21:29.101733Z","iopub.status.idle":"2024-03-21T08:21:29.106371Z","shell.execute_reply":"2024-03-21T08:21:29.105285Z","shell.execute_reply.started":"2024-03-21T08:21:29.101988Z"},"trusted":true},"outputs":[],"source":["np.set_printoptions(precision=4)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:29.107643Z","iopub.status.busy":"2024-03-21T08:21:29.107375Z","iopub.status.idle":"2024-03-21T08:21:41.062102Z","shell.execute_reply":"2024-03-21T08:21:41.060979Z","shell.execute_reply.started":"2024-03-21T08:21:29.107621Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataframes MITBIH correctly read into workspace\n"]}],"source":["# This cell now makes use of the downloadfolder for the datasets. If already available locally, the filepaths can be changed.\n","df_train= pd.read_csv(path_to_datasets + \"/\" + 'mitbih_train.csv', header=None)\n","df_test=pd.read_csv(path_to_datasets + \"/\" +  'mitbih_test.csv',header=None)\n","print(\"Dataframes MITBIH correctly read into workspace\")\n","\n","#split target and value\n","train_target=df_train[187]\n","test_target=df_test[187]\n","train=df_train.drop(187,axis=1)\n","test=df_test.drop(187,axis=1)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.065807Z","iopub.status.busy":"2024-03-21T08:21:41.065436Z","iopub.status.idle":"2024-03-21T08:21:41.072551Z","shell.execute_reply":"2024-03-21T08:21:41.071545Z","shell.execute_reply.started":"2024-03-21T08:21:41.065770Z"},"trusted":true},"outputs":[],"source":["#Switches to decide the dataset sampling method and which models should be run\n","class Config_Sampling:\n","    oversample = False #equals to B_SMOTE\n","    undersample = False\n","    sample_name = \"UNDEFINED_SAMPLE\"\n","    \n","    \n","\n","Train_Simple_ANN = True #Trains the simple ANN\n","Train_Simple_CNN = False #Trains the simple CNN\n","Train_Advanced_CNN = False #Trains the advanced CNN\n"," "]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.074675Z","iopub.status.busy":"2024-03-21T08:21:41.074106Z","iopub.status.idle":"2024-03-21T08:21:41.088102Z","shell.execute_reply":"2024-03-21T08:21:41.087129Z","shell.execute_reply.started":"2024-03-21T08:21:41.074641Z"},"trusted":true},"outputs":[],"source":["oversampler = SMOTE()\n","undersampler = RandomUnderSampler()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.089884Z","iopub.status.busy":"2024-03-21T08:21:41.089546Z","iopub.status.idle":"2024-03-21T08:21:41.099952Z","shell.execute_reply":"2024-03-21T08:21:41.099018Z","shell.execute_reply.started":"2024-03-21T08:21:41.089853Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using the original mitbih dataset\n","Sample Name: MITBIH_A_Original\n"]}],"source":["#Based on the configuration in the Config_Sampling Class, the datasets are sampled and the sample name is modified accordingly\n","if Config_Sampling.oversample:\n","    train, train_target = oversampler.fit_resample(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n","    Config_Sampling.sample_name = \"MITBIH_B_SMOTE\"\n","    print(\"Sample Name:\", Config_Sampling.sample_name)\n","elif Config_Sampling.undersample:\n","    train, train_target = undersampler.fit_resample(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n","    Config_Sampling.sample_name = \"MITBIH_C_RUS\"\n","    print(\"Sample Name:\", Config_Sampling.sample_name)\n","else: \n","    print(\"Using the original mitbih dataset\")\n","    Config_Sampling.sample_name = \"MITBIH_A_Original\"\n","    print(\"Sample Name:\", Config_Sampling.sample_name)"]},{"cell_type":"markdown","metadata":{},"source":["# **Deep Learning Models**"]},{"cell_type":"markdown","metadata":{},"source":["## **Simple Artificial neural Network**\n","ANN without convolutional layers. Only Dense layers are used. No Pooling, Flattening or Dropping out. Base model for later comparison."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.133496Z","iopub.status.busy":"2024-03-21T08:21:41.133136Z","iopub.status.idle":"2024-03-21T08:21:41.158193Z","shell.execute_reply":"2024-03-21T08:21:41.157083Z","shell.execute_reply.started":"2024-03-21T08:21:41.133465Z"},"trusted":true},"outputs":[],"source":["if Train_Simple_ANN == True:\n","    class Config_ANN:\n","        epochs = 70 #70 is default (exp1)\n","        batch_size = 10 #10 is default (exp1)\n","        exp_name = 4 #Experiment Number counter \n","        patience = 70 #10 # is default (exp1)\n","        initial_learning_rate=0.001 #Default Initial Learning Rate for ADAM: 0.001\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n","        reduce_lr_every_10_epochs = True #Reduce the learning rate each 10 Epochs with the rate defined below. Default is False for exp1.\n","        lr_reduction_rate = 0.5 # Reduction Rate for reducing each 10 epochs. 0.5 means, that lr is halfed each 10 epochs.\n","        filepath_checkpoint = '/kaggle/working/Simple_ANN/'+str(Config_Sampling.sample_name) +'/experiment_'+str(exp_name) + '_'+str (Config_Sampling.sample_name) +'.weights.h5'\n","        filepath_accuracy_plot = '/kaggle/working/Simple_ANN/'+str(Config_Sampling.sample_name) +'/experiment_'+str(exp_name) +  '_'+ str (Config_Sampling.sample_name) +'.accuracy_plot.png'\n","        filepath_loss_plot = '/kaggle/working/Simple_ANN/'+str(Config_Sampling.sample_name) +'/experiment_'+str(exp_name) +  '_'+str (Config_Sampling.sample_name) +'.loss_plot.png'\n","        filepath_classification_report = '/kaggle/working/Simple_ANN/'+str(Config_Sampling.sample_name) +'/experiment_'+str(exp_name) +  '_'+str (Config_Sampling.sample_name) +'.classification_report.txt'\n","\n","    #Model structure: This is not changed during experiments.\n","    ann_model = tf.keras.models.Sequential()\n","    ann_model.add(tf.keras.layers.Dense(60, activation=tf.keras.layers.LeakyReLU(alpha=0.001), input_shape=(187,)))\n","    ann_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    ann_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    ann_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    ann_model.add(tf.keras.layers.Dense(5, activation='softmax'))\n","\n","    #Function for the learning rate scheduler\n","    def lr_scheduler(epoch, lr):\n","        if Config_ANN.reduce_lr_every_10_epochs and epoch % 10 == 0: # %10 == 0 means each 10 epochs (only then no \"rest\" after dividing trough 10)\n","            return lr * Config_ANN.lr_reduction_rate  # reduce the learning rate with the configured rate\n","        else:\n","            return lr\n","\n","    # callback for the learning rate reduction schedule (function)\n","    lr_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n","\n","    #Model checkpoint: Saves the best model for all epochs with regards to the validation accuracy. Only weights (.h5) are saved.\n","    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","        filepath=Config_ANN.filepath_checkpoint,\n","        save_weights_only=True,\n","        monitor='val_accuracy',\n","        mode='max',\n","        save_best_only=True)\n","    \n","    #Early stop callback: If validation accuracy does not change during the last XXX epochs, training is stopped (XXX is configured as patience)\n","    early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=Config_ANN.patience)\n","\n","    #Compilation of model with ADAM and custom lr, sparse_categorical_crossentropy since we have integers as class labels.\n","    ann_model.compile(optimizer=Config_ANN.optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","    ann_model.summary() #Model summary for usage in reports or other files.\n","\n","    #training the model and storing all results in training history. aside from the datasets, all arguments are called from our config-class. Also calling the respective callbacks.\n","    ann_model_history = ann_model.fit(train, train_target, epochs=Config_ANN.epochs, batch_size = Config_ANN.batch_size, validation_data = (test, test_target), callbacks=[model_checkpoint_callback, early_stop_callback, lr_scheduler_callback])\n","\n","    #some plots for early interpretion during the run\n","    plt.plot(ann_model_history.history['accuracy'])\n","    plt.plot(ann_model_history.history['val_accuracy'])\n","    plt.legend([\"accuracy\",\"val_accuracy\"])\n","    plt.title('Accuracy Vs Val_Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.savefig(Config_ANN.filepath_accuracy_plot)\n","    plt.show()\n","    plt.close()\n","\n","\n","    plt.plot(ann_model_history.history['loss'])\n","    plt.plot(ann_model_history.history['val_loss'])\n","    plt.legend([\"loss\",\"val_loss\"])\n","    plt.title('Loss Vs Val_loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.savefig(Config_ANN.filepath_loss_plot)\n","    plt.show()\n","    plt.close()\n","\n","    #Predict on test set.\n","    predictions = ann_model.predict(test).argmax(axis=1) #directly getting classes instead of probabilities.\n","\n","    #make classification report and save it directly as a file.\n","    report=classification_report(test_target, predictions, digits=4)\n","    print(report)\n","    with open(Config_ANN.filepath_classification_report, 'w') as report_file:\n","        report_file.write(report)\n","\n","    print(\"EVERYTHING FINISHED FOR SIMPLE ANN MODEL!\")\n","\n","else:\n","    print(\"Simple ANN Model is not trained and evaluated\")\n","    "]},{"cell_type":"markdown","metadata":{},"source":["## **Simple Convolutional neural Network**\n","CNN with one convolutional layer. The same Dense layers from Simple ANN Model are used and the first Dense Layer is replaced by a convolutional layer. No Pooling, Flattening or Dropping out. Base model for later comparison. We use a Conv1D layer, because our Input Data is one-dimensional (i.e. is a timeseries). We have sequential and not spatial data.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.159917Z","iopub.status.busy":"2024-03-21T08:21:41.159540Z","iopub.status.idle":"2024-03-21T08:21:41.192546Z","shell.execute_reply":"2024-03-21T08:21:41.191633Z","shell.execute_reply.started":"2024-03-21T08:21:41.159874Z"},"trusted":true},"outputs":[],"source":["if Train_Simple_CNN == True:\n","    class Config_CNN:\n","        epochs = 70 #70 is default (exp1)\n","        batch_size = 10 #10 is default (exp1)\n","        exp_name = 4 #Experiment Number counter \n","        patience = 70 #10 # is default (exp1)\n","        initial_learning_rate=0.001 #Default Initial Learning Rate for ADAM: 0.001\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n","        reduce_lr_every_10_epochs = True #Reduce the learning rate each 10 Epochs with the rate defined below. Default is False for exp1.\n","        lr_reduction_rate = 0.5 # Reduction Rate for reducing each 10 epochs. 0.5 means, that lr is halfed each 10 epochs.\n","        Conv1_filter_num = 32 # Number of filters in the convolutional layer (more means more shape-variations can be detected) \n","        Conv1_filter_size = 3 # Size (e.g. 3 by 3) of single convolutional kernel. More means a more rough approach to detection of patterns.\n","        filepath_checkpoint = '/kaggle/working/Simple_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.weights.h5'\n","        filepath_accuracy_plot = '/kaggle/working/Simple_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.accuracy_plot.png'\n","        filepath_loss_plot = '/kaggle/working/Simple_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.loss_plot.png'\n","        filepath_classification_report = '/kaggle/working/Simple_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.classification_report.txt'\n","\n","    #Model structure: This is not changed during experiments.\n","    cnn_model = tf.keras.models.Sequential()\n","    cnn_model.add(tf.keras.layers.Conv1D(Config_CNN.Conv1_filter_num, Config_CNN.Conv1_filter_size, activation='relu', input_shape=(187, 1))) # We add one Conv1D layer to the model\n","    cnn_model.add(tf.keras.layers.Flatten()) # After \n","    cnn_model.add(tf.keras.layers.Dense(60, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    cnn_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    cnn_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    cnn_model.add(tf.keras.layers.Dense(5, activation='softmax'))\n","    \n","    #Function for the learning rate scheduler\n","    def lr_scheduler(epoch, lr):\n","        if Config_ANN.reduce_lr_every_10_epochs and epoch % 10 == 0: # %10 == 0 means each 10 epochs (only then no \"rest\" after dividing trough 10)\n","            return lr * Config_ANN.lr_reduction_rate  # reduce the learning rate with the configured rate\n","        else:\n","            return lr\n","\n","    # callback for the learning rate reduction schedule (function)\n","    lr_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n","    \n","    #Model checkpoint: Saves the best model for all epochs with regards to the validation accuracy. Only weights (.h5) are saved.\n","    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","        filepath=Config_CNN.filepath_checkpoint,\n","        save_weights_only=True,\n","        monitor='val_accuracy',\n","        mode='max',\n","        save_best_only=True)\n","    \n","    #Early stop callback: If validation accuracy does not change during the last XXX epochs, training is stopped (XXX is configured as patience)\n","    early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=Config_CNN.patience)\n","\n","    #Compilation of model with ADAM and custom lr, sparse_categorical_crossentropy since we have integers as class labels.\n","    cnn_model.compile(optimizer=Config_CNN.optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","    cnn_model.summary()\n","\n","    \"\"\"The following transformation of DataFrames to NumPy arrays and reshaping them is necessary to match the input requirements of the Convolutional Neural Network model.\n","     CNNs in TensorFlow typically expect input data in the form of NumPy arrays with specific shapes, especially when using Conv1D layers.\n","    This reshaping ensures that the data is in the correct format for training and inference.\"\"\"\n","    train_array = train.to_numpy()\n","    test_array = test.to_numpy()\n","    train_reshaped = train_array.reshape(train_array.shape[0], train_array.shape[1], 1)\n","    test_reshaped = test_array.reshape(test_array.shape[0], test_array.shape[1], 1)\n","        \n","    #training the model and storing all results in training history. aside from the datasets, all arguments are called from our config-class. Also calling the respective callbacks.\n","    cnn_model_history = cnn_model.fit(train_reshaped, train_target, epochs=Config_CNN.epochs, batch_size=Config_CNN.batch_size, validation_data=(test_reshaped, test_target), callbacks=[model_checkpoint_callback, early_stop_callback, lr_scheduler_callback])\n","    \n","    #some plots for early interpretion during the run\n","    plt.plot(cnn_model_history.history['accuracy'])\n","    plt.plot(cnn_model_history.history['val_accuracy'])\n","    plt.legend([\"accuracy\",\"val_accuracy\"])\n","    plt.title(f'Accuracy Vs Val_Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.savefig(Config_CNN.filepath_accuracy_plot)\n","    plt.show()\n","    plt.close()\n","\n","    plt.plot(cnn_model_history.history['loss'])\n","    plt.plot(cnn_model_history.history['val_loss'])\n","    plt.legend([\"loss\",\"val_loss\"])\n","    plt.title('Loss Vs Val_loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.savefig(Config_CNN.filepath_loss_plot)\n","    plt.show()\n","    plt.close()\n","\n","    #Predict on test set.\n","    predictions = cnn_model.predict(test_reshaped).argmax(axis=1)\n","\n","    #make classification report and save it directly as a file.\n","    report = classification_report(test_target, predictions, digits=4)\n","    print(report)\n","    with open(Config_CNN.filepath_classification_report, 'w') as report_file:\n","        report_file.write(report)\n","\n","    print(\"EVERYTHING FINISHED FOR SIMPLE CNN MODEL!\")\n","\n","else:\n","    print(\"Simple CNN Model is not trained and evaluated\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## **Advanced CNN**\n","\n","This is the Model from Hakan's notebook, it will be adapted so that the model structure can be used with exp1 to exp4.\n","\n","<font color='red'>Hakan can do some notes here ################</font>\n","\n","**Most important change(s):**\n","- Sparse_Categorical_Crossentropy instead of categorical: Should not change metrics but removes the need for one-hot encoding the dataset, which is not done in this notebook. We have numbers of 0 to 4 for each category, so it's wondering why normal categorical_cross_entropy even worked in the original notebook from Hakan?\n","- By using Sparse_Categorical_Crossentropy, I had to adjust the making of the classification report: test_target is now used directly and not with .argmax(axis=1). I also don't quite understand why this should be used in any way because test_target is already a set of predicted classes and doesn't need to be further processed.\n","- Removed the plateau learning rate callback; we can use this in experiment5, but it was not used during exp1 to 4"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.194485Z","iopub.status.busy":"2024-03-21T08:21:41.193895Z","iopub.status.idle":"2024-03-21T08:23:07.748097Z","shell.execute_reply":"2024-03-21T08:23:07.747016Z","shell.execute_reply.started":"2024-03-21T08:21:41.194389Z"},"trusted":true},"outputs":[],"source":["if Train_Advanced_CNN == True:\n","    class Config_Advanced_CNN:\n","        epochs = 70 #70 is default (exp1)\n","        batch_size = 10 #10 is default (exp1)\n","        exp_name = 4 #Experiment Number counter \n","        patience = 70 #10 # is default (exp1)\n","        initial_learning_rate=0.001 #Default Initial Learning Rate for ADAM: 0.001\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n","        reduce_lr_every_10_epochs = True #Reduce the learning rate each 10 Epochs with the rate defined below. Default is False for exp1.\n","        lr_reduction_rate = 0.5 # Reduction Rate for reducing each 10 epochs. 0.5 means, that lr is halfed each 10 epochs.\n","        Conv1_filter_num = 32 # Number of filters in the convolutional layer (more means more shape-variations can be detected) \n","        Conv1_filter_size = 3 # Size (e.g. 3 by 3) of single convolutional kernel. More means a more rough approach to detection of patterns.\n","        filepath_checkpoint = '/kaggle/working/Advanced_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.weights.h5'\n","        filepath_accuracy_plot = '/kaggle/working/Advanced_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.accuracy_plot.png'\n","        filepath_loss_plot = '/kaggle/working/Advanced_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.loss_plot.png'\n","        filepath_classification_report = '/kaggle/working/Advanced_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.classification_report.txt'\n","\n","    #Model structure: This is not changed during experiments.\n","    adv_cnn_model = tf.keras.models.Sequential()\n","    adv_cnn_model.add(tf.keras.layers.Conv1D(Config_Advanced_CNN.Conv1_filter_num, Config_Advanced_CNN.Conv1_filter_size, activation='relu', input_shape=(187, 1))) # We add one Conv1D layer to the model\n","    adv_cnn_model.add(tf.keras.layers.MaxPooling1D(pool_size=3, strides=2)) # We add one Conv1D layer to the model\n","    adv_cnn_model.add(tf.keras.layers.Conv1D(Config_Advanced_CNN.Conv1_filter_num//2, Config_Advanced_CNN.Conv1_filter_size, activation='relu' )) # We add one Conv1D layer to the model\n","    adv_cnn_model.add(tf.keras.layers.MaxPooling1D(pool_size=3, strides=2)) # We add one Conv1D layer to the model\n","    adv_cnn_model.add(tf.keras.layers.Flatten()) # After  \n","    adv_cnn_model.add(tf.keras.layers.Dropout(rate=0.2))\n","    adv_cnn_model.add(tf.keras.layers.Dense(120, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    adv_cnn_model.add(tf.keras.layers.Dense(60, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    adv_cnn_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    adv_cnn_model.add(tf.keras.layers.Dense(5, activation='softmax'))\n","    \n","    #Function for the learning rate scheduler\n","    def lr_scheduler(epoch, lr):\n","        if Config_Advanced_CNN.reduce_lr_every_10_epochs and epoch % 10 == 0: # %10 == 0 means each 10 epochs (only then no \"rest\" after dividing trough 10)\n","            return lr * Config_Advanced_CNN.lr_reduction_rate  # reduce the learning rate with the configured rate\n","        else:\n","            return lr\n","\n","    # callback for the learning rate reduction schedule (function)\n","    lr_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n","    \n","    #Model checkpoint: Saves the best model for all epochs with regards to the validation accuracy. Only weights (.h5) are saved.\n","    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","        filepath=Config_Advanced_CNN.filepath_checkpoint,\n","        save_weights_only=True,\n","        monitor='val_accuracy',\n","        mode='max',\n","        save_best_only=True)\n","    \n","    #Early stop callback: If validation accuracy does not change during the last XXX epochs, training is stopped (XXX is configured as patience)\n","    early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=Config_Advanced_CNN.patience)\n","\n","    #Compilation of model with ADAM and custom lr, sparse_categorical_crossentropy since we have integers as class labels.\n","    adv_cnn_model.compile(optimizer=Config_Advanced_CNN.optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","    adv_cnn_model.summary()\n","\n","    \n","    \"\"\"The following transformation of DataFrames to NumPy arrays and reshaping them is necessary to match the input requirements of the Convolutional Neural Network model.\n","     CNNs in TensorFlow typically expect input data in the form of NumPy arrays with specific shapes, especially when using Conv1D layers.\n","    This reshaping ensures that the data is in the correct format for training and inference.\"\"\"\n","    train_array = train.to_numpy()\n","    test_array = test.to_numpy()\n","    train_reshaped = train_array.reshape(train_array.shape[0], train_array.shape[1], 1)\n","    test_reshaped = test_array.reshape(test_array.shape[0], test_array.shape[1], 1)\n","    \n","    #training the model and storing all results in training history. aside from the datasets, all arguments are called from our config-class. Also calling the respective callbacks.\n","    adv_cnn_model_history = adv_cnn_model.fit(train_reshaped, train_target, epochs=Config_Advanced_CNN.epochs, batch_size=Config_Advanced_CNN.batch_size, \n","                                      validation_data=(test_reshaped, test_target), \n","                                      callbacks=[model_checkpoint_callback, early_stop_callback, lr_scheduler_callback])\n","    \n","    #some plots for early interpretion during the run\n","    plt.plot(adv_cnn_model_history.history['accuracy'])\n","    plt.plot(adv_cnn_model_history.history['val_accuracy'])\n","    plt.legend([\"accuracy\",\"val_accuracy\"])\n","    plt.title(f'Accuracy Vs Val_Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.savefig(Config_Advanced_CNN.filepath_accuracy_plot)\n","    plt.show()\n","    plt.close()\n","\n","    plt.plot(adv_cnn_model_history.history['loss'])\n","    plt.plot(adv_cnn_model_history.history['val_loss'])\n","    plt.legend([\"loss\",\"val_loss\"])\n","    plt.title('Loss Vs Val_loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.savefig(Config_Advanced_CNN.filepath_loss_plot)\n","    plt.show()\n","    plt.close()\n","\n","    #Predict on test set.\n","    predictions = adv_cnn_model.predict(test_reshaped).argmax(axis=1)\n","      \n","    #make classification report and save it directly as a file.\n","    report = classification_report(test_target, predictions, digits=4)\n","    print(report)\n","    with open(Config_Advanced_CNN.filepath_classification_report, 'w') as report_file:\n","        report_file.write(report)\n","\n","    print(\"EVERYTHING FINISHED FOR ADVANCED CNN MODEL!\")\n","\n","else:\n","    print(\"ADVANCED CNN Model is not trained and evaluated\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":29414,"sourceId":37484,"sourceType":"datasetVersion"}],"dockerImageVersionId":30626,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0rc1"}},"nbformat":4,"nbformat_minor":4}
