{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-21T08:21:29.078990Z","iopub.status.busy":"2024-03-21T08:21:29.078141Z","iopub.status.idle":"2024-03-21T08:21:29.100221Z","shell.execute_reply":"2024-03-21T08:21:29.099372Z","shell.execute_reply.started":"2024-03-21T08:21:29.078956Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU is used\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra \n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from sklearn.model_selection import train_test_split\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","from sklearn.metrics import accuracy_score, classification_report\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","if tf.config.experimental.list_physical_devices('GPU'):\n","    print('GPU is used')\n","else:\n","    print('no GPU available, CPU is used instead')\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:29.102011Z","iopub.status.busy":"2024-03-21T08:21:29.101733Z","iopub.status.idle":"2024-03-21T08:21:29.106371Z","shell.execute_reply":"2024-03-21T08:21:29.105285Z","shell.execute_reply.started":"2024-03-21T08:21:29.101988Z"},"trusted":true},"outputs":[],"source":["np.set_printoptions(precision=4)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:29.107643Z","iopub.status.busy":"2024-03-21T08:21:29.107375Z","iopub.status.idle":"2024-03-21T08:21:41.062102Z","shell.execute_reply":"2024-03-21T08:21:41.060979Z","shell.execute_reply.started":"2024-03-21T08:21:29.107621Z"},"trusted":true},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/heartbeat/mitbih_train.csv'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Read in the datasets for MITBIH\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_train\u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/heartbeat/mitbih_train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df_test\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/heartbeat/mitbih_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataframes MITBIH correctly read into workspace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n","File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/heartbeat/mitbih_train.csv'"]}],"source":["#Read in the datasets for MITBIH\n","df_train= pd.read_csv('/kaggle/input/heartbeat/mitbih_train.csv', header=None)\n","df_test=pd.read_csv('/kaggle/input/heartbeat/mitbih_test.csv',header=None)\n","print(\"Dataframes MITBIH correctly read into workspace\")\n","\n","#split target and value\n","train_target=df_train[187]\n","test_target=df_test[187]\n","train=df_train.drop(187,axis=1)\n","test=df_test.drop(187,axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.065807Z","iopub.status.busy":"2024-03-21T08:21:41.065436Z","iopub.status.idle":"2024-03-21T08:21:41.072551Z","shell.execute_reply":"2024-03-21T08:21:41.071545Z","shell.execute_reply.started":"2024-03-21T08:21:41.065770Z"},"trusted":true},"outputs":[],"source":["#Switches to decide the dataset sampling method and which models should be run\n","class Config_Sampling:\n","    oversample = True #equals to B_SMOTE\n","    undersample = False\n","    sample_name = \"UNDEFINED_SAMPLE\"\n","    \n","    \n","\n","Train_Simple_ANN = False #Trains the simple ANN\n","Train_Simple_CNN = False #Trains the simple CNN\n","Train_Advanced_CNN = True #Trains the advanced CNN\n"," "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.074675Z","iopub.status.busy":"2024-03-21T08:21:41.074106Z","iopub.status.idle":"2024-03-21T08:21:41.088102Z","shell.execute_reply":"2024-03-21T08:21:41.087129Z","shell.execute_reply.started":"2024-03-21T08:21:41.074641Z"},"trusted":true},"outputs":[],"source":["oversampler = SMOTE()\n","undersampler = RandomUnderSampler()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.089884Z","iopub.status.busy":"2024-03-21T08:21:41.089546Z","iopub.status.idle":"2024-03-21T08:21:41.099952Z","shell.execute_reply":"2024-03-21T08:21:41.099018Z","shell.execute_reply.started":"2024-03-21T08:21:41.089853Z"},"trusted":true},"outputs":[],"source":["#Based on the configuration in the Config_Sampling Class, the datasets are sampled and the sample name is modified accordingly\n","if Config_Sampling.oversample:\n","    train, train_target = oversampler.fit_resample(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n","    Config_Sampling.sample_name = \"MITBIH_B_SMOTE\"\n","    print(\"Sample Name:\", Config_Sampling.sample_name)\n","elif Config_Sampling.undersample:\n","    train, train_target = undersampler.fit_resample(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n","    Config_Sampling.sample_name = \"MITBIH_C_RUS\"\n","    print(\"Sample Name:\", Config_Sampling.sample_name)\n","else: \n","    print(\"Using the original mitbih dataset\")\n","    Config_Sampling.sample_name = \"MITBIH_A_Original\"\n","    print(\"Sample Name:\", Config_Sampling.sample_name)"]},{"cell_type":"markdown","metadata":{},"source":["# **Deep Learning Models**"]},{"cell_type":"markdown","metadata":{},"source":["## **Simple Artificial neural Network**\n","ANN without convolutional layers. Only Dense layers are used. No Pooling, Flattening or Dropping out. Base model for later comparison."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.133496Z","iopub.status.busy":"2024-03-21T08:21:41.133136Z","iopub.status.idle":"2024-03-21T08:21:41.158193Z","shell.execute_reply":"2024-03-21T08:21:41.157083Z","shell.execute_reply.started":"2024-03-21T08:21:41.133465Z"},"trusted":true},"outputs":[],"source":["if Train_Simple_ANN == True:\n","    class Config_ANN:\n","        epochs = 70 #70 is default (exp1)\n","        batch_size = 10 #10 is default (exp1)\n","        exp_name = 4 #Experiment Number counter \n","        patience = 70 #10 # is default (exp1)\n","        initial_learning_rate=0.001 #Default Initial Learning Rate for ADAM: 0.001\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n","        reduce_lr_every_10_epochs = True #Reduce the learning rate each 10 Epochs with the rate defined below. Default is False for exp1.\n","        lr_reduction_rate = 0.5 # Reduction Rate for reducing each 10 epochs. 0.5 means, that lr is halfed each 10 epochs.\n","        filepath_checkpoint = '/kaggle/working/Simple_ANN/'+str(Config_Sampling.sample_name) +'/experiment_'+str(exp_name) + '_'+str (Config_Sampling.sample_name) +'.weights.h5'\n","        filepath_accuracy_plot = '/kaggle/working/Simple_ANN/'+str(Config_Sampling.sample_name) +'/experiment_'+str(exp_name) +  '_'+ str (Config_Sampling.sample_name) +'.accuracy_plot.png'\n","        filepath_loss_plot = '/kaggle/working/Simple_ANN/'+str(Config_Sampling.sample_name) +'/experiment_'+str(exp_name) +  '_'+str (Config_Sampling.sample_name) +'.loss_plot.png'\n","        filepath_classification_report = '/kaggle/working/Simple_ANN/'+str(Config_Sampling.sample_name) +'/experiment_'+str(exp_name) +  '_'+str (Config_Sampling.sample_name) +'.classification_report.txt'\n","\n","    #Model structure: This is not changed during experiments.\n","    ann_model = tf.keras.models.Sequential()\n","    ann_model.add(tf.keras.layers.Dense(60, activation=tf.keras.layers.LeakyReLU(alpha=0.001), input_shape=(187,)))\n","    ann_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    ann_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    ann_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    ann_model.add(tf.keras.layers.Dense(5, activation='softmax'))\n","\n","    #Function for the learning rate scheduler\n","    def lr_scheduler(epoch, lr):\n","        if Config_ANN.reduce_lr_every_10_epochs and epoch % 10 == 0: # %10 == 0 means each 10 epochs (only then no \"rest\" after dividing trough 10)\n","            return lr * Config_ANN.lr_reduction_rate  # reduce the learning rate with the configured rate\n","        else:\n","            return lr\n","\n","    # callback for the learning rate reduction schedule (function)\n","    lr_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n","\n","    #Model checkpoint: Saves the best model for all epochs with regards to the validation accuracy. Only weights (.h5) are saved.\n","    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","        filepath=Config_ANN.filepath_checkpoint,\n","        save_weights_only=True,\n","        monitor='val_accuracy',\n","        mode='max',\n","        save_best_only=True)\n","    \n","    #Early stop callback: If validation accuracy does not change during the last XXX epochs, training is stopped (XXX is configured as patience)\n","    early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=Config_ANN.patience)\n","\n","    #Compilation of model with ADAM and custom lr, sparse_categorical_crossentropy since we have integers as class labels.\n","    ann_model.compile(optimizer=Config_ANN.optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","    ann_model.summary() #Model summary for usage in reports or other files.\n","\n","    #training the model and storing all results in training history. aside from the datasets, all arguments are called from our config-class. Also calling the respective callbacks.\n","    ann_model_history = ann_model.fit(train, train_target, epochs=Config_ANN.epochs, batch_size = Config_ANN.batch_size, validation_data = (test, test_target), callbacks=[model_checkpoint_callback, early_stop_callback, lr_scheduler_callback])\n","\n","    #some plots for early interpretion during the run\n","    plt.plot(ann_model_history.history['accuracy'])\n","    plt.plot(ann_model_history.history['val_accuracy'])\n","    plt.legend([\"accuracy\",\"val_accuracy\"])\n","    plt.title('Accuracy Vs Val_Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.savefig(Config_ANN.filepath_accuracy_plot)\n","    plt.show()\n","    plt.close()\n","\n","\n","    plt.plot(ann_model_history.history['loss'])\n","    plt.plot(ann_model_history.history['val_loss'])\n","    plt.legend([\"loss\",\"val_loss\"])\n","    plt.title('Loss Vs Val_loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.savefig(Config_ANN.filepath_loss_plot)\n","    plt.show()\n","    plt.close()\n","\n","    #Predict on test set.\n","    predictions = ann_model.predict(test).argmax(axis=1) #directly getting classes instead of probabilities.\n","\n","    #make classification report and save it directly as a file.\n","    report=classification_report(test_target, predictions, digits=4)\n","    print(report)\n","    with open(Config_ANN.filepath_classification_report, 'w') as report_file:\n","        report_file.write(report)\n","\n","    print(\"EVERYTHING FINISHED FOR SIMPLE ANN MODEL!\")\n","\n","else:\n","    print(\"Simple ANN Model is not trained and evaluated\")\n","    "]},{"cell_type":"markdown","metadata":{},"source":["## **Simple Convolutional neural Network**\n","CNN with one convolutional layer. The same Dense layers from Simple ANN Model are used and the first Dense Layer is replaced by a convolutional layer. No Pooling, Flattening or Dropping out. Base model for later comparison. We use a Conv1D layer, because our Input Data is one-dimensional (i.e. is a timeseries). We have sequential and not spatial data.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.159917Z","iopub.status.busy":"2024-03-21T08:21:41.159540Z","iopub.status.idle":"2024-03-21T08:21:41.192546Z","shell.execute_reply":"2024-03-21T08:21:41.191633Z","shell.execute_reply.started":"2024-03-21T08:21:41.159874Z"},"trusted":true},"outputs":[],"source":["if Train_Simple_CNN == True:\n","    class Config_CNN:\n","        epochs = 70 #70 is default (exp1)\n","        batch_size = 10 #10 is default (exp1)\n","        exp_name = 4 #Experiment Number counter \n","        patience = 70 #10 # is default (exp1)\n","        initial_learning_rate=0.001 #Default Initial Learning Rate for ADAM: 0.001\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n","        reduce_lr_every_10_epochs = True #Reduce the learning rate each 10 Epochs with the rate defined below. Default is False for exp1.\n","        lr_reduction_rate = 0.5 # Reduction Rate for reducing each 10 epochs. 0.5 means, that lr is halfed each 10 epochs.\n","        Conv1_filter_num = 32 # Number of filters in the convolutional layer (more means more shape-variations can be detected) \n","        Conv1_filter_size = 3 # Size (e.g. 3 by 3) of single convolutional kernel. More means a more rough approach to detection of patterns.\n","        filepath_checkpoint = '/kaggle/working/Simple_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.weights.h5'\n","        filepath_accuracy_plot = '/kaggle/working/Simple_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.accuracy_plot.png'\n","        filepath_loss_plot = '/kaggle/working/Simple_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.loss_plot.png'\n","        filepath_classification_report = '/kaggle/working/Simple_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.classification_report.txt'\n","\n","    #Model structure: This is not changed during experiments.\n","    cnn_model = tf.keras.models.Sequential()\n","    cnn_model.add(tf.keras.layers.Conv1D(Config_CNN.Conv1_filter_num, Config_CNN.Conv1_filter_size, activation='relu', input_shape=(187, 1))) # We add one Conv1D layer to the model\n","    cnn_model.add(tf.keras.layers.Flatten()) # After \n","    cnn_model.add(tf.keras.layers.Dense(60, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    cnn_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    cnn_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    cnn_model.add(tf.keras.layers.Dense(5, activation='softmax'))\n","    \n","    #Function for the learning rate scheduler\n","    def lr_scheduler(epoch, lr):\n","        if Config_ANN.reduce_lr_every_10_epochs and epoch % 10 == 0: # %10 == 0 means each 10 epochs (only then no \"rest\" after dividing trough 10)\n","            return lr * Config_ANN.lr_reduction_rate  # reduce the learning rate with the configured rate\n","        else:\n","            return lr\n","\n","    # callback for the learning rate reduction schedule (function)\n","    lr_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n","    \n","    #Model checkpoint: Saves the best model for all epochs with regards to the validation accuracy. Only weights (.h5) are saved.\n","    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","        filepath=Config_CNN.filepath_checkpoint,\n","        save_weights_only=True,\n","        monitor='val_accuracy',\n","        mode='max',\n","        save_best_only=True)\n","    \n","    #Early stop callback: If validation accuracy does not change during the last XXX epochs, training is stopped (XXX is configured as patience)\n","    early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=Config_CNN.patience)\n","\n","    #Compilation of model with ADAM and custom lr, sparse_categorical_crossentropy since we have integers as class labels.\n","    cnn_model.compile(optimizer=Config_CNN.optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","    cnn_model.summary()\n","\n","    \"\"\"The following transformation of DataFrames to NumPy arrays and reshaping them is necessary to match the input requirements of the Convolutional Neural Network model.\n","     CNNs in TensorFlow typically expect input data in the form of NumPy arrays with specific shapes, especially when using Conv1D layers.\n","    This reshaping ensures that the data is in the correct format for training and inference.\"\"\"\n","    train_array = train.to_numpy()\n","    test_array = test.to_numpy()\n","    train_reshaped = train_array.reshape(train_array.shape[0], train_array.shape[1], 1)\n","    test_reshaped = test_array.reshape(test_array.shape[0], test_array.shape[1], 1)\n","        \n","    #training the model and storing all results in training history. aside from the datasets, all arguments are called from our config-class. Also calling the respective callbacks.\n","    cnn_model_history = cnn_model.fit(train_reshaped, train_target, epochs=Config_CNN.epochs, batch_size=Config_CNN.batch_size, validation_data=(test_reshaped, test_target), callbacks=[model_checkpoint_callback, early_stop_callback, lr_scheduler_callback])\n","    \n","    #some plots for early interpretion during the run\n","    plt.plot(cnn_model_history.history['accuracy'])\n","    plt.plot(cnn_model_history.history['val_accuracy'])\n","    plt.legend([\"accuracy\",\"val_accuracy\"])\n","    plt.title(f'Accuracy Vs Val_Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.savefig(Config_CNN.filepath_accuracy_plot)\n","    plt.show()\n","    plt.close()\n","\n","    plt.plot(cnn_model_history.history['loss'])\n","    plt.plot(cnn_model_history.history['val_loss'])\n","    plt.legend([\"loss\",\"val_loss\"])\n","    plt.title('Loss Vs Val_loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.savefig(Config_CNN.filepath_loss_plot)\n","    plt.show()\n","    plt.close()\n","\n","    #Predict on test set.\n","    predictions = cnn_model.predict(test_reshaped).argmax(axis=1)\n","\n","    #make classification report and save it directly as a file.\n","    report = classification_report(test_target, predictions, digits=4)\n","    print(report)\n","    with open(Config_CNN.filepath_classification_report, 'w') as report_file:\n","        report_file.write(report)\n","\n","    print(\"EVERYTHING FINISHED FOR SIMPLE CNN MODEL!\")\n","\n","else:\n","    print(\"Simple CNN Model is not trained and evaluated\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## **Advanced CNN**\n","\n","This is the Model from Hakan's notebook, it will be adapted so that the model structure can be used with exp1 to exp4.\n","\n","<font color='red'>Hakan can do some notes here ################</font>\n","\n","**Most important change(s):**\n","- Sparse_Categorical_Crossentropy instead of categorical: Should not change metrics but removes the need for one-hot encoding the dataset, which is not done in this notebook. We have numbers of 0 to 4 for each category, so it's wondering why normal categorical_cross_entropy even worked in the original notebook from Hakan?\n","- By using Sparse_Categorical_Crossentropy, I had to adjust the making of the classification report: test_target is now used directly and not with .argmax(axis=1). I also don't quite understand why this should be used in any way because test_target is already a set of predicted classes and doesn't need to be further processed.\n","- Removed the plateau learning rate callback; we can use this in experiment5, but it was not used during exp1 to 4"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.194485Z","iopub.status.busy":"2024-03-21T08:21:41.193895Z","iopub.status.idle":"2024-03-21T08:23:07.748097Z","shell.execute_reply":"2024-03-21T08:23:07.747016Z","shell.execute_reply.started":"2024-03-21T08:21:41.194389Z"},"trusted":true},"outputs":[],"source":["if Train_Advanced_CNN == True:\n","    class Config_Advanced_CNN:\n","        epochs = 70 #70 is default (exp1)\n","        batch_size = 10 #10 is default (exp1)\n","        exp_name = 4 #Experiment Number counter \n","        patience = 70 #10 # is default (exp1)\n","        initial_learning_rate=0.001 #Default Initial Learning Rate for ADAM: 0.001\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n","        reduce_lr_every_10_epochs = True #Reduce the learning rate each 10 Epochs with the rate defined below. Default is False for exp1.\n","        lr_reduction_rate = 0.5 # Reduction Rate for reducing each 10 epochs. 0.5 means, that lr is halfed each 10 epochs.\n","        Conv1_filter_num = 32 # Number of filters in the convolutional layer (more means more shape-variations can be detected) \n","        Conv1_filter_size = 3 # Size (e.g. 3 by 3) of single convolutional kernel. More means a more rough approach to detection of patterns.\n","        filepath_checkpoint = '/kaggle/working/Advanced_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.weights.h5'\n","        filepath_accuracy_plot = '/kaggle/working/Advanced_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.accuracy_plot.png'\n","        filepath_loss_plot = '/kaggle/working/Advanced_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.loss_plot.png'\n","        filepath_classification_report = '/kaggle/working/Advanced_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.classification_report.txt'\n","\n","    #Model structure: This is not changed during experiments.\n","    adv_cnn_model = tf.keras.models.Sequential()\n","    adv_cnn_model.add(tf.keras.layers.Conv1D(Config_Advanced_CNN.Conv1_filter_num, Config_Advanced_CNN.Conv1_filter_size, activation='relu', input_shape=(187, 1))) # We add one Conv1D layer to the model\n","    adv_cnn_model.add(tf.keras.layers.MaxPooling1D(pool_size=3, strides=2)) # We add one Conv1D layer to the model\n","    adv_cnn_model.add(tf.keras.layers.Conv1D(Config_Advanced_CNN.Conv1_filter_num//2, Config_Advanced_CNN.Conv1_filter_size, activation='relu' )) # We add one Conv1D layer to the model\n","    adv_cnn_model.add(tf.keras.layers.MaxPooling1D(pool_size=3, strides=2)) # We add one Conv1D layer to the model\n","    adv_cnn_model.add(tf.keras.layers.Flatten()) # After  \n","    adv_cnn_model.add(tf.keras.layers.Dropout(rate=0.2))\n","    adv_cnn_model.add(tf.keras.layers.Dense(120, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    adv_cnn_model.add(tf.keras.layers.Dense(60, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    adv_cnn_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    adv_cnn_model.add(tf.keras.layers.Dense(5, activation='softmax'))\n","    \n","    #Function for the learning rate scheduler\n","    def lr_scheduler(epoch, lr):\n","        if Config_Advanced_CNN.reduce_lr_every_10_epochs and epoch % 10 == 0: # %10 == 0 means each 10 epochs (only then no \"rest\" after dividing trough 10)\n","            return lr * Config_Advanced_CNN.lr_reduction_rate  # reduce the learning rate with the configured rate\n","        else:\n","            return lr\n","\n","    # callback for the learning rate reduction schedule (function)\n","    lr_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n","    \n","    #Model checkpoint: Saves the best model for all epochs with regards to the validation accuracy. Only weights (.h5) are saved.\n","    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","        filepath=Config_Advanced_CNN.filepath_checkpoint,\n","        save_weights_only=True,\n","        monitor='val_accuracy',\n","        mode='max',\n","        save_best_only=True)\n","    \n","    #Early stop callback: If validation accuracy does not change during the last XXX epochs, training is stopped (XXX is configured as patience)\n","    early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=Config_Advanced_CNN.patience)\n","\n","    #Compilation of model with ADAM and custom lr, sparse_categorical_crossentropy since we have integers as class labels.\n","    adv_cnn_model.compile(optimizer=Config_Advanced_CNN.optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","    adv_cnn_model.summary()\n","\n","    \n","    \"\"\"The following transformation of DataFrames to NumPy arrays and reshaping them is necessary to match the input requirements of the Convolutional Neural Network model.\n","     CNNs in TensorFlow typically expect input data in the form of NumPy arrays with specific shapes, especially when using Conv1D layers.\n","    This reshaping ensures that the data is in the correct format for training and inference.\"\"\"\n","    train_array = train.to_numpy()\n","    test_array = test.to_numpy()\n","    train_reshaped = train_array.reshape(train_array.shape[0], train_array.shape[1], 1)\n","    test_reshaped = test_array.reshape(test_array.shape[0], test_array.shape[1], 1)\n","    \n","    #training the model and storing all results in training history. aside from the datasets, all arguments are called from our config-class. Also calling the respective callbacks.\n","    adv_cnn_model_history = adv_cnn_model.fit(train_reshaped, train_target, epochs=Config_Advanced_CNN.epochs, batch_size=Config_Advanced_CNN.batch_size, \n","                                      validation_data=(test_reshaped, test_target), \n","                                      callbacks=[model_checkpoint_callback, early_stop_callback, lr_scheduler_callback])\n","    \n","    #some plots for early interpretion during the run\n","    plt.plot(adv_cnn_model_history.history['accuracy'])\n","    plt.plot(adv_cnn_model_history.history['val_accuracy'])\n","    plt.legend([\"accuracy\",\"val_accuracy\"])\n","    plt.title(f'Accuracy Vs Val_Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.savefig(Config_Advanced_CNN.filepath_accuracy_plot)\n","    plt.show()\n","    plt.close()\n","\n","    plt.plot(adv_cnn_model_history.history['loss'])\n","    plt.plot(adv_cnn_model_history.history['val_loss'])\n","    plt.legend([\"loss\",\"val_loss\"])\n","    plt.title('Loss Vs Val_loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.savefig(Config_Advanced_CNN.filepath_loss_plot)\n","    plt.show()\n","    plt.close()\n","\n","    #Predict on test set.\n","    predictions = adv_cnn_model.predict(test_reshaped).argmax(axis=1)\n","      \n","    #make classification report and save it directly as a file.\n","    report = classification_report(test_target, predictions, digits=4)\n","    print(report)\n","    with open(Config_Advanced_CNN.filepath_classification_report, 'w') as report_file:\n","        report_file.write(report)\n","\n","    print(\"EVERYTHING FINISHED FOR ADVANCED CNN MODEL!\")\n","\n","else:\n","    print(\"ADVANCED CNN Model is not trained and evaluated\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":29414,"sourceId":37484,"sourceType":"datasetVersion"}],"dockerImageVersionId":30626,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0rc1"}},"nbformat":4,"nbformat_minor":4}
