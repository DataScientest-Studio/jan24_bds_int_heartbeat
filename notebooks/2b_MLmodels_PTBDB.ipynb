{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ee4ebc0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-08T19:44:40.180264Z",
     "iopub.status.busy": "2024-03-08T19:44:40.179864Z",
     "iopub.status.idle": "2024-03-08T19:44:43.390911Z",
     "shell.execute_reply": "2024-03-08T19:44:43.389524Z"
    },
    "papermill": {
     "duration": 3.228191,
     "end_time": "2024-03-08T19:44:43.393819",
     "exception": false,
     "start_time": "2024-03-08T19:44:40.165628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pickle\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "import xgboost as XGB\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed72fbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/simon/.kaggle/kaggle.json'\n",
      "All Datasets are already available.\n"
     ]
    }
   ],
   "source": [
    "#In this cell, the datasets are downloaded via the KaggleAPI directly from the source. It might be necessary to authentificate first via Webbrowser to make this work.\n",
    "#FUrthermore, a folder ../data is created, which is on the .gitignore list. In this folder, large files >100mb and the original datasets MITBIH and PTBDB are stored.\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "#configuring and authentification with kaggle api. This could be configured so that a authentification mask is shown?\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "#Configuring the metadata for the ecg heartbeat data (original username etc)\n",
    "dataset_owner = \"shayanfazeli\"\n",
    "dataset_name = \"heartbeat\"\n",
    "\n",
    "#Configuring a download path that is NOT in the current github repo (so the big files are not pushed and cause an error!) --> Links to filepaths have to be dynamically adjusted\n",
    "download_path = \"../data/KAGGLE_datasets\" #In this case we use the data folder that is in the .gitignore list and therefore not pushed! To keep everything in one local repo.\n",
    "\n",
    "# Download structure: First check if dataset is already downloaded, else download it and store it in download path (should be outside git repo!)\n",
    "dataset_folder = os.path.join(download_path, dataset_name)\n",
    "if not os.path.exists(dataset_folder):\n",
    "    # Case 1: Dataset path is not created --> Create it and download datasets into it\n",
    "    api.dataset_download_files(dataset_owner + \"/\" + dataset_name, path=download_path + \"/\" + dataset_name, unzip=True)\n",
    "    print(\"Datasets are downloaded and unzipped.\")\n",
    "else:\n",
    "    # Case 2: Folder is created, but datasets might be missing\n",
    "    missing_files = [] \n",
    "    for file_name in [\"mitbih_test.csv\", \"mitbih_train.csv\", \"ptbdb_abnormal.csv\", \"ptbdb_normal.csv\"]:  # These are the hardcoded names of the datasets that should be downloaded.\n",
    "        file_path = os.path.join(dataset_folder, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            missing_files.append(file_name)\n",
    "\n",
    "    if missing_files:\n",
    "        # If the list contains missing files, download ALL files and overwrite the old folder.\n",
    "        api.dataset_download_files(dataset_owner + \"/\" + dataset_name, path=download_path + \"/\" + dataset_name, unzip=True, force=True)\n",
    "        print(\"Missing data was donwloaded and unzipped. All Datasets are now available.\")\n",
    "    else:\n",
    "        print(\"All Datasets are already available.\")\n",
    "\n",
    "#Creating new variable that links to the datasets and can be used in the rest of the code.\n",
    "path_to_datasets = download_path + \"/\" + dataset_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a63a020a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T19:44:43.419755Z",
     "iopub.status.busy": "2024-03-08T19:44:43.419203Z",
     "iopub.status.idle": "2024-03-08T19:44:43.424620Z",
     "shell.execute_reply": "2024-03-08T19:44:43.423364Z"
    },
    "papermill": {
     "duration": 0.021105,
     "end_time": "2024-03-08T19:44:43.426745",
     "exception": false,
     "start_time": "2024-03-08T19:44:43.405640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#only 4 digits are printed out by numpy calculations.\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb7976fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T19:44:43.452214Z",
     "iopub.status.busy": "2024-03-08T19:44:43.451260Z",
     "iopub.status.idle": "2024-03-08T19:44:45.018326Z",
     "shell.execute_reply": "2024-03-08T19:44:45.017357Z"
    },
    "papermill": {
     "duration": 1.582728,
     "end_time": "2024-03-08T19:44:45.021165",
     "exception": false,
     "start_time": "2024-03-08T19:44:43.438437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell now makes use of the downloadfolder for the datasets. If already available locally, the filepaths can be changed.\n",
    "ptbdb_normal= pd.read_csv(path_to_datasets + \"/\" +  'ptbdb_normal.csv', header=None)\n",
    "ptbdb_abnormal=pd.read_csv(path_to_datasets + \"/\" +   'ptbdb_abnormal.csv',header=None)\n",
    "\n",
    "#PTBDB comes preconfigures as normal and abnormal dataset, so we concate both, reshuffle them and then generate the test and train sets.\n",
    "ptbdb_combined = pd.concat([ptbdb_normal, ptbdb_abnormal], ignore_index=True, axis=0)\n",
    "\n",
    "#Reshuffle the whole new dataframe\n",
    "ptbdb_combined_shuffled = ptbdb_combined.sample(frac=1, random_state=42)\n",
    "\n",
    "#Generate Test and Train datasets\n",
    "X = ptbdb_combined_shuffled.iloc[:, :-1] #All values except the last column\n",
    "y = ptbdb_combined_shuffled.iloc[:, -1] #All values from the last column\n",
    "\n",
    "train, test, train_target, test_target = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b47f4078",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T19:44:45.046590Z",
     "iopub.status.busy": "2024-03-08T19:44:45.045632Z",
     "iopub.status.idle": "2024-03-08T19:44:45.050665Z",
     "shell.execute_reply": "2024-03-08T19:44:45.049737Z"
    },
    "papermill": {
     "duration": 0.019945,
     "end_time": "2024-03-08T19:44:45.052854",
     "exception": false,
     "start_time": "2024-03-08T19:44:45.032909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Switches for the user to define which sample method is used and which models are run.\n",
    "class Config:\n",
    "    oversample = False #refers to PTBDB B_SMOTE\n",
    "    undersample = False #Refers to undersampling with random undersampler\n",
    "    sample_name = \"UNDEFINED_SAMPLE\"\n",
    "\n",
    "Train_SVM =  False #trains the SVM Model without Gridsearch\n",
    "Train_KNN = False #trains the KNN Model without Gridsearch\n",
    "Train_DTC = False #trains the DTC Model without Gridsearch\n",
    "Train_RF = False #trains the RF Model without Gridsearch\n",
    "Train_XGB = False #trains the XGB Model without Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd5480da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T19:44:45.077988Z",
     "iopub.status.busy": "2024-03-08T19:44:45.076910Z",
     "iopub.status.idle": "2024-03-08T19:44:45.082093Z",
     "shell.execute_reply": "2024-03-08T19:44:45.081306Z"
    },
    "papermill": {
     "duration": 0.019846,
     "end_time": "2024-03-08T19:44:45.084238",
     "exception": false,
     "start_time": "2024-03-08T19:44:45.064392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "oversampler = SMOTE()\n",
    "undersampler = RandomUnderSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "76f8e1da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T19:44:45.108902Z",
     "iopub.status.busy": "2024-03-08T19:44:45.108209Z",
     "iopub.status.idle": "2024-03-08T19:44:45.402040Z",
     "shell.execute_reply": "2024-03-08T19:44:45.400786Z"
    },
    "papermill": {
     "duration": 0.309274,
     "end_time": "2024-03-08T19:44:45.404886",
     "exception": false,
     "start_time": "2024-03-08T19:44:45.095612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the original PTBDB dataset\n",
      "Sample Name: PTBDB_A_Original\n"
     ]
    }
   ],
   "source": [
    "#Based on the user settings, Resampling is done and the sample name (i.e. filenames) are modified.\n",
    "if Config.oversample:\n",
    "    train, train_target = oversampler.fit_resample(train, train_target)\n",
    "    Config.sample_name = \"PTBDB_B_SMOTE\"\n",
    "    print(\"Sample Name:\", Config.sample_name)\n",
    "elif Config.undersample:\n",
    "    train, train_target = undersampler.fit_resample(train, train_target)\n",
    "    Config.sample_name = \"PTBDB_C_RUS\"\n",
    "    print(\"Sample Name:\", Config.sample_name)\n",
    "else: \n",
    "    print(\"Using the original PTBDB dataset\")\n",
    "    Config.sample_name = \"PTBDB_A_Original\"\n",
    "    print(\"Sample Name:\", Config.sample_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf34119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to save models and classification report directly after running.\n",
    "def save_model_and_report(model, report, model_filename, report_filename, model_folder=\"../models/ML_Models\", report_folder=\"../reports/figures/ML_Models\"):\n",
    "    # Save the model\n",
    "    model_savepath = os.path.join(model_folder, model_filename)\n",
    "    with open(model_savepath, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"The model was saved as {model_filename} in folder {model_folder}.\")\n",
    "\n",
    "    # Check if model file size is greater than 98MB (Restriction for github only!)\n",
    "    if os.path.getsize(model_savepath) > 98 * 1024 * 1024:  # Check if size is greater than 98MB\n",
    "        print(\"Model file size is too big. Changing save path...\")\n",
    "        model_folder = \"../data/models_too_big_for_git\"\n",
    "        os.makedirs(model_folder, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "        new_model_savepath = os.path.join(model_folder, model_filename)\n",
    "        os.replace(model_savepath, new_model_savepath)  # Move the model to the new location\n",
    "        print(f\"Model moved to {model_folder} due to its size.\")\n",
    "\n",
    "    # Save the classification report\n",
    "    report_savepath = os.path.join(report_folder, report_filename)\n",
    "    with open(report_savepath, \"w\") as f:\n",
    "        f.write(report)\n",
    "    print(f\"The classification report was saved as {report_filename} in folder {report_folder}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "45018274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T19:44:45.429746Z",
     "iopub.status.busy": "2024-03-08T19:44:45.428967Z",
     "iopub.status.idle": "2024-03-08T19:44:45.437443Z",
     "shell.execute_reply": "2024-03-08T19:44:45.436329Z"
    },
    "papermill": {
     "duration": 0.023587,
     "end_time": "2024-03-08T19:44:45.439990",
     "exception": false,
     "start_time": "2024-03-08T19:44:45.416403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11641, 187)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcf7de9",
   "metadata": {
    "papermill": {
     "duration": 0.011083,
     "end_time": "2024-03-08T19:44:45.462683",
     "exception": false,
     "start_time": "2024-03-08T19:44:45.451600",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **SVM**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "563b6a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model withoud gridsearch is not trained\n"
     ]
    }
   ],
   "source": [
    "#Just the code for model creation, fitting and creating the report out of the predictions.\n",
    "if Train_SVM == True:\n",
    "    model = SVC(cache_size=500)\n",
    "    model.fit(train,train_target)\n",
    "    predictions = model.predict(test)\n",
    "    report=classification_report(test_target, predictions, digits=4)\n",
    "    print(report)\n",
    "    #Calling the save_model_and_report function for the SVM Model\n",
    "    model_filename = f\"SVM_Basemodel_no_gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"SVM_Basemodel_no_gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    save_model_and_report(model, report, model_filename, report_filename)\n",
    "else:\n",
    "    print(\"SVM Model withoud gridsearch is not trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f4bb81",
   "metadata": {},
   "source": [
    "# **KNN**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ece6adb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Model without gridsearch is not trained.\n"
     ]
    }
   ],
   "source": [
    "if Train_KNN == True:\n",
    "    model = KNN(n_jobs = -1)\n",
    "    model.fit(train,train_target)\n",
    "    predictions = model.predict(test)\n",
    "    report=classification_report(test_target, predictions, digits=4)\n",
    "    print(report)\n",
    "    #Calling the save_model_and_report function for the KNN Model\n",
    "    model_filename = f\"KNN_Basemodel_no_gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"KNN_Basemodel_no_gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    save_model_and_report(model, report, model_filename, report_filename)\n",
    "else:\n",
    "    print(\"KNN Model without gridsearch is not trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5682fbe",
   "metadata": {},
   "source": [
    "# **Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a3219cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Model without gridsearch is not trained.\n"
     ]
    }
   ],
   "source": [
    "if Train_DTC == True:\n",
    "    model = DTC()\n",
    "    model.fit(train,train_target)\n",
    "    predictions = model.predict(test)\n",
    "    report=classification_report(test_target, predictions, digits=4)\n",
    "    print(report)\n",
    "    #Calling the save_model_and_report function for the Decision Tree Model\n",
    "    model_filename = f\"DTC_Basemodel_no_gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"DTC_Basemodel_no_gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    save_model_and_report(model, report, model_filename, report_filename)\n",
    "else:\n",
    "    print(\"Decision Tree Model without gridsearch is not trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4816091b",
   "metadata": {},
   "source": [
    "# **Random Forest**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e063687d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model without gridsearch is not trained.\n"
     ]
    }
   ],
   "source": [
    "if Train_RF == True:\n",
    "    model = RFC(n_jobs = -1)\n",
    "    model.fit(train,train_target)\n",
    "    predictions = model.predict(test)\n",
    "    report=classification_report(test_target, predictions, digits=4)\n",
    "    print(report)\n",
    "    #Calling the save_model_and_report function for the Random Forest Model\n",
    "    model_filename = f\"RFC_Basemodel_no_gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"RFC_Basemodel_no_gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    save_model_and_report(model, report, model_filename, report_filename)\n",
    "else:\n",
    "    print(\"Random Forest Model without gridsearch is not trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18561269",
   "metadata": {},
   "source": [
    "# **XGBoost**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ed7698c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XBG Model without gridsearch is not trained.\n"
     ]
    }
   ],
   "source": [
    "if Train_XGB == True:\n",
    "    model = XGB.XGBClassifier(objective='binary:logistic')\n",
    "    model.fit(train,train_target)\n",
    "    predictions = model.predict(test)\n",
    "    report=classification_report(test_target, predictions, digits=4)\n",
    "    print(report)\n",
    "    #Calling the save_model_and_report function for the XGB Model\n",
    "    model_filename = f\"XGB_Basemodel_no_gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"XGB_Basemodel_no_gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    save_model_and_report(model, report, model_filename, report_filename)\n",
    "else:\n",
    "    print(\"XBG Model without gridsearch is not trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6479ca80",
   "metadata": {},
   "source": [
    "# Gridsearch Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f86acb9",
   "metadata": {},
   "source": [
    "Gridsearches were only performed for the Mitbih Dataset in our reports. For further testing (if the jury wants or the project is continued) we include the code to theoretically perform gridsearches on the PTBDB Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "492dd946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary imports\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import neighbors\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "78d916ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Dataset that is used for the gridsearch: PTBDB_A_Original\n"
     ]
    }
   ],
   "source": [
    "# Configuration switches / Paramgrids\n",
    "Gridsearch_SVM = False # (Done by Hakan?)\n",
    "Gridsearch_KNN = False # (Done by Alex)\n",
    "Gridsearch_DTC = False # (Done by Alex)\n",
    "Gridsearch_RFC = False # (Done by Simon)\n",
    "Gridsearch_XGB = False # (Done by Hakan)\n",
    "\n",
    "#print the current dataset that is in usage:\n",
    "print(\"Current Dataset that is used for the gridsearch:\", Config.sample_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d31743",
   "metadata": {},
   "source": [
    "## Gridsearch SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "65136c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Gridsearch for SVM is performed.\n"
     ]
    }
   ],
   "source": [
    "if Gridsearch_SVM == True:\n",
    "    svm_model = SVC()\n",
    "    params = {'C': [1, 10, 100], 'kernel': [ 'linear','rbf']}\n",
    "    grid_search = GridSearchCV(estimator=svm_model, param_grid=params, verbose=3, cv=5, n_jobs=-1)\n",
    "    grid_search.fit(train, train_target)\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    y_pred = grid_search.predict(test)\n",
    "    print(pd.crosstab(test_target, y_pred, rownames=['Class Actual'], colnames=['Class Predict']))\n",
    "    print(classification_report(test_target, y_pred))\n",
    "\n",
    "    print(\"Accuracy Score on Test Set: \", grid_search.score(test, test_target))\n",
    "    model_filename = f\"SVM_Optimized_Model_with_Gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"SVM_Optimized_Model_with_Gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    # Saving the best model and the associated classification report\n",
    "    save_model_and_report(grid_search.best_estimator_, classification_report(test_target, y_pred), model_filename, report_filename)\n",
    "\n",
    "else:\n",
    "    print(\"No Gridsearch for SVM is performed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d69c6d",
   "metadata": {},
   "source": [
    "## Gridsearch KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fd03d625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Gridsearch for KNN is performed.\n"
     ]
    }
   ],
   "source": [
    "if Gridsearch_KNN == True:\n",
    "    knn = neighbors.KNeighborsClassifier(n_jobs=-1)\n",
    "    param_knn = {'metric': ['manhattan','minkowski','chebyshev','euclidean'],'n_neighbors': [k for  k in range(1, 7)]}\n",
    "    grid=GridSearchCV(estimator=knn,param_grid=param_knn,verbose=3, cv=5 , scoring='accuracy', n_jobs=-1)\n",
    "    grid.fit(train,train_target)\n",
    "    print(pd.DataFrame(grid.cv_results_)[['params', 'mean_test_score', 'std_test_score']])\n",
    "\n",
    "    print('Best Metric:', grid.best_estimator_.get_params()['metric'])\n",
    "    print('Best K:', grid.best_estimator_.get_params()['n_neighbors'])\n",
    "\n",
    "    y_pred=grid.predict(test)\n",
    "    print(pd.crosstab(test_target,y_pred,rownames=['Class Actual'],colnames=['Class Predict']))\n",
    "    print(classification_report(test_target,y_pred))\n",
    "\n",
    "\n",
    "    print(\"Accuracy Score on Test Set: \", grid.score(test, test_target))\n",
    "    model_filename = f\"KNN_Optimized_Model_with_Gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"KNN_Optimized_Model_with_Gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    #saving the best model and the associated classification report\n",
    "    save_model_and_report(grid.best_estimator_, classification_report(test_target, y_pred), model_filename, report_filename)\n",
    "else:\n",
    "    print(\"No Gridsearch for KNN is performed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978005a7",
   "metadata": {},
   "source": [
    "## Gridsearch DTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f6c40a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Gridsearch for DTC is performed.\n"
     ]
    }
   ],
   "source": [
    "if Gridsearch_DTC == True:\n",
    "    dt=DecisionTreeClassifier()\n",
    "    param_dt={'criterion': ['gini', 'entropy'],   'max_depth': [2,4,6,8,10,12]} \n",
    "    grid=GridSearchCV(estimator=dt,param_grid=param_dt, verbose=3, cv=5 , scoring='accuracy', n_jobs=-1)\n",
    "    grid.fit(train, train_target)\n",
    "    pd.DataFrame(grid.cv_results_)[['params', 'mean_test_score', 'std_test_score']]\n",
    "\n",
    "    print('Best Metric:', grid.best_estimator_.get_params()['criterion'])\n",
    "    print('Best Depth:', grid.best_estimator_.get_params()['max_depth'])\n",
    "\n",
    "    y_pred=grid.predict(test)\n",
    "    print(pd.crosstab(test_target,y_pred,rownames=['Class Actual'],colnames=['Class Predict']))\n",
    "    print(classification_report(test_target,y_pred))\n",
    "\n",
    "    print(\"Accuracy Score on Test Set: \", grid.score(test, test_target))\n",
    "    model_filename = f\"DTC_Optimized_Model_with_Gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"DTC_Optimized_Model_with_Gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    #saving the best model and the associated classification report\n",
    "    save_model_and_report(grid.best_estimator_, classification_report(test_target, y_pred), model_filename, report_filename)\n",
    "else:\n",
    "    print(\"No Gridsearch for DTC is performed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb03bcd0",
   "metadata": {},
   "source": [
    "## Gridsearch RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "39408a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Gridsearch for RFC is performed.\n"
     ]
    }
   ],
   "source": [
    "if Gridsearch_RFC == True:\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],  \n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5], \n",
    "        'min_samples_leaf': [1, 2, 4], \n",
    "        'max_features': ['sqrt', 'log2']    \n",
    "    }\n",
    "    rfc_grid = RandomForestClassifier(n_jobs=-1)\n",
    "    grid_search = GridSearchCV(estimator = rfc_grid, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs = -1, verbose=3)\n",
    "    grid_search.fit(train, train_target)\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    y_pred=grid_search.predict(test)\n",
    "    print(pd.crosstab(test_target,y_pred,rownames=['Class Actual'],colnames=['Class Predict']))\n",
    "    print(classification_report(test_target,y_pred))\n",
    "\n",
    "    print(\"Accuracy Score on Test Set: \", grid_search.score(test, test_target))\n",
    "    model_filename = f\"RFC_Optimized_Model_with_Gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"RFC_Optimized_Model_with_Gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    #saving the best model and the associated classification report\n",
    "    save_model_and_report(grid_search.best_estimator_, classification_report(test_target, y_pred), model_filename, report_filename)\n",
    "\n",
    "else:\n",
    "    print(\"No Gridsearch for RFC is performed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f37cc",
   "metadata": {},
   "source": [
    "## Gridsearch XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a72bee7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Gridsearch for XBG is performed.\n"
     ]
    }
   ],
   "source": [
    "if Gridsearch_XGB == True:\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',  # for binary classification\n",
    "        n_jobs=-1,\n",
    "        #max_depth=10,\n",
    "        #learning_rate=0.01,\n",
    "        #n_estimators=100\n",
    "    )\n",
    "    params={'max_depth':[50], 'n_estimators':[1500, 2000, 2500], 'learning_rate': [ 0.1, 0.2, 0.5]}\n",
    "    grid_search = GridSearchCV(estimator=xgb_model,param_grid=params, verbose=3, cv=5, n_jobs=-1)\n",
    "    grid_search.fit(train, train_target)\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    y_pred=grid_search.predict(test)\n",
    "    print(pd.crosstab(test_target,y_pred,rownames=['Class Actual'],colnames=['Class Predict']))\n",
    "    print(classification_report(test_target,y_pred))\n",
    "\n",
    "    print(\"Accuracy Score on Test Set: \", grid_search.score(test, test_target))\n",
    "    model_filename = f\"XGB_Optimized_Model_with_Gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"XGB_Optimized_Model_with_Gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    #saving the best model and the associated classification report\n",
    "    save_model_and_report(grid_search.best_estimator_, classification_report(test_target, y_pred), model_filename, report_filename)\n",
    "\n",
    "else:\n",
    "    print(\"No Gridsearch for XBG is performed.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 29414,
     "sourceId": 37484,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 55.638252,
   "end_time": "2024-03-08T19:45:31.353184",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-08T19:44:35.714932",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
