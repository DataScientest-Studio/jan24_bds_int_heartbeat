{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-21T08:21:29.078990Z","iopub.status.busy":"2024-03-21T08:21:29.078141Z","iopub.status.idle":"2024-03-21T08:21:29.100221Z","shell.execute_reply":"2024-03-21T08:21:29.099372Z","shell.execute_reply.started":"2024-03-21T08:21:29.078956Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-25 14:04:15.647038: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-03-25 14:04:15.647134: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-03-25 14:04:15.710133: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-03-25 14:04:15.827121: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-03-25 14:04:17.505812: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]},{"name":"stdout","output_type":"stream","text":["GPU is used\n"]},{"name":"stderr","output_type":"stream","text":["2024-03-25 14:04:19.760138: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2024-03-25 14:04:19.953436: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2024-03-25 14:04:19.953519: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra \n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from sklearn.model_selection import train_test_split\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","from sklearn.metrics import accuracy_score, classification_report\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","if tf.config.experimental.list_physical_devices('GPU'):\n","    print('GPU is used')\n","else:\n","    print('no GPU available, CPU is used instead')\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:29.102011Z","iopub.status.busy":"2024-03-21T08:21:29.101733Z","iopub.status.idle":"2024-03-21T08:21:29.106371Z","shell.execute_reply":"2024-03-21T08:21:29.105285Z","shell.execute_reply.started":"2024-03-21T08:21:29.101988Z"},"trusted":true},"outputs":[],"source":["np.set_printoptions(precision=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:29.107643Z","iopub.status.busy":"2024-03-21T08:21:29.107375Z","iopub.status.idle":"2024-03-21T08:21:41.062102Z","shell.execute_reply":"2024-03-21T08:21:41.060979Z","shell.execute_reply.started":"2024-03-21T08:21:29.107621Z"},"trusted":true},"outputs":[],"source":["df_train= pd.read_csv('/kaggle/input/heartbeat/mitbih_train.csv', header=None)\n","df_test=pd.read_csv('/kaggle/input/heartbeat/mitbih_test.csv',header=None)\n","print(\"Dataframes MITBIH correctly read into workspace\")\n","\n","#split target and value\n","train_target=df_train[187]\n","test_target=df_test[187]\n","train=df_train.drop(187,axis=1)\n","test=df_test.drop(187,axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.065807Z","iopub.status.busy":"2024-03-21T08:21:41.065436Z","iopub.status.idle":"2024-03-21T08:21:41.072551Z","shell.execute_reply":"2024-03-21T08:21:41.071545Z","shell.execute_reply.started":"2024-03-21T08:21:41.065770Z"},"trusted":true},"outputs":[],"source":["class Config_Sampling:\n","    oversample = True #equals to B_SMOTE\n","    undersample = False\n","    sample_name = \"UNDEFINED_SAMPLE\"\n","    \n","    \n","\n","Train_Simple_ANN = False #Trains the simple ANN\n","Train_Simple_CNN = False #Trains the simple CNN\n","Train_Advanced_CNN = True #Trains the advanced CNN\n"," "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.074675Z","iopub.status.busy":"2024-03-21T08:21:41.074106Z","iopub.status.idle":"2024-03-21T08:21:41.088102Z","shell.execute_reply":"2024-03-21T08:21:41.087129Z","shell.execute_reply.started":"2024-03-21T08:21:41.074641Z"},"trusted":true},"outputs":[],"source":["oversampler = SMOTE()\n","undersampler = RandomUnderSampler()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.089884Z","iopub.status.busy":"2024-03-21T08:21:41.089546Z","iopub.status.idle":"2024-03-21T08:21:41.099952Z","shell.execute_reply":"2024-03-21T08:21:41.099018Z","shell.execute_reply.started":"2024-03-21T08:21:41.089853Z"},"trusted":true},"outputs":[],"source":["if Config_Sampling.oversample:\n","    train, train_target = oversampler.fit_resample(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n","    Config_Sampling.sample_name = \"MITBIH_B_SMOTE\"\n","    print(\"Sample Name:\", Config_Sampling.sample_name)\n","elif Config_Sampling.undersample:\n","    train, train_target = undersampler.fit_resample(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n","    Config_Sampling.sample_name = \"MITBIH_C_RUS\"\n","    print(\"Sample Name:\", Config_Sampling.sample_name)\n","else: \n","    print(\"Using the original mitbih dataset\")\n","    Config_Sampling.sample_name = \"MITBIH_A_Original\"\n","    print(\"Sample Name:\", Config_Sampling.sample_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.102201Z","iopub.status.busy":"2024-03-21T08:21:41.101950Z","iopub.status.idle":"2024-03-21T08:21:41.129922Z","shell.execute_reply":"2024-03-21T08:21:41.128825Z","shell.execute_reply.started":"2024-03-21T08:21:41.102179Z"},"trusted":true},"outputs":[],"source":["#overview for later comparison (has the model produced the right output?)\n","print(\"Initial Train shape\", train.shape)\n","print(\"Initial Test shape\", test.shape)\n","print(\"Initial Value count for train dataset (target):\", train_target.value_counts())\n","print(\"Initial Value count for test dataset (target):\", test_target.value_counts())\n","\n","print(\"Initial percentages for train dataset (target):\", 100* train_target.value_counts() / len(train_target))\n","print(\"Initial percentages for test dataset (target):\", 100 *test_target.value_counts() / len(test_target))"]},{"cell_type":"markdown","metadata":{},"source":["# **Deep Learning Models**"]},{"cell_type":"markdown","metadata":{},"source":["## **Simple Artificial neural Network**\n","ANN without convolutional layers. Only Dense layers are used. No Pooling, Flattening or Dropping out. Base model for later comparison."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.133496Z","iopub.status.busy":"2024-03-21T08:21:41.133136Z","iopub.status.idle":"2024-03-21T08:21:41.158193Z","shell.execute_reply":"2024-03-21T08:21:41.157083Z","shell.execute_reply.started":"2024-03-21T08:21:41.133465Z"},"trusted":true},"outputs":[],"source":["if Train_Simple_ANN == True:\n","    class Config_ANN:\n","        epochs = 70 #70\n","        batch_size = 10 #10\n","        exp_name = 4 #Please WRITE THIS DOWN IN EXCEL FILE (With screenshot)\n","        patience = 70 #10 # for early stopping\n","        initial_learning_rate=0.001 #Initial Learning Rate for ADAM: 0.001\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n","        reduce_lr_every_10_epochs = True #Reduce the learning rate each 10 Epochs with the rate defined below\n","        lr_reduction_rate = 0.5 # Reduction Rate for reducing each 10 epochs. 0.5 means, that lr is halfed each 10 epochs.\n","        filepath_checkpoint = '/kaggle/working/Simple_ANN/'+str(Config_Sampling.sample_name) +'/experiment_'+str(exp_name) + '_'+str (Config_Sampling.sample_name) +'.weights.h5'\n","        filepath_accuracy_plot = '/kaggle/working/Simple_ANN/'+str(Config_Sampling.sample_name) +'/experiment_'+str(exp_name) +  '_'+ str (Config_Sampling.sample_name) +'.accuracy_plot.png'\n","        filepath_loss_plot = '/kaggle/working/Simple_ANN/'+str(Config_Sampling.sample_name) +'/experiment_'+str(exp_name) +  '_'+str (Config_Sampling.sample_name) +'.loss_plot.png'\n","        filepath_classification_report = '/kaggle/working/Simple_ANN/'+str(Config_Sampling.sample_name) +'/experiment_'+str(exp_name) +  '_'+str (Config_Sampling.sample_name) +'.classification_report.txt'\n","\n","    ann_model = tf.keras.models.Sequential()\n","    ann_model.add(tf.keras.layers.Dense(60, activation=tf.keras.layers.LeakyReLU(alpha=0.001), input_shape=(187,)))\n","    ann_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    ann_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    ann_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    ann_model.add(tf.keras.layers.Dense(5, activation='softmax'))\n","\n","    def lr_scheduler(epoch, lr):\n","        if Config_ANN.reduce_lr_every_10_epochs and epoch % 10 == 0: # %10 == 0 means each 10 epochs (only then no \"rest\" after dividing trough 10)\n","            return lr * Config_ANN.lr_reduction_rate  # reduce the learning rate with the configured rate\n","        else:\n","            return lr\n","\n","    # callback for the learning rate reduction schedule\n","    lr_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n","\n","    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","        filepath=Config_ANN.filepath_checkpoint,\n","        save_weights_only=True,\n","        monitor='val_accuracy',\n","        mode='max',\n","        save_best_only=True)\n","    \n","    early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=Config_ANN.patience)\n","\n","    ann_model.compile(optimizer=Config_ANN.optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","    ann_model.summary()\n","\n","    ann_model_history = ann_model.fit(train, train_target, epochs=Config_ANN.epochs, batch_size = Config_ANN.batch_size, validation_data = (test, test_target), callbacks=[model_checkpoint_callback, early_stop_callback, lr_scheduler_callback])\n","\n","    plt.plot(ann_model_history.history['accuracy'])\n","    plt.plot(ann_model_history.history['val_accuracy'])\n","    plt.legend([\"accuracy\",\"val_accuracy\"])\n","    plt.title('Accuracy Vs Val_Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.savefig(Config_ANN.filepath_accuracy_plot)\n","    plt.show()\n","    plt.close()\n","\n","\n","    plt.plot(ann_model_history.history['loss'])\n","    plt.plot(ann_model_history.history['val_loss'])\n","    plt.legend([\"loss\",\"val_loss\"])\n","    plt.title('Loss Vs Val_loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.savefig(Config_ANN.filepath_loss_plot)\n","    plt.show()\n","    plt.close()\n","\n","    predictions = ann_model.predict(test).argmax(axis=1) #directly getting classes instead of probabilities.\n","\n","    report=classification_report(test_target, predictions, digits=4)\n","    print(report)\n","    with open(Config_ANN.filepath_classification_report, 'w') as report_file:\n","        report_file.write(report)\n","\n","    print(\"EVERYTHING FINISHED FOR SIMPLE ANN MODEL!\")\n","\n","else:\n","    print(\"Simple ANN Model is not trained and evaluated\")\n","    "]},{"cell_type":"markdown","metadata":{},"source":["## **Simple Convolutional neural Network**\n","CNN with convolutional layers. The same Dense layers from Simple ANN Model are used. No Pooling, Flattening or Dropping out. Base model for later comparison.\n","\n","*Notes: We could try different variations of flattening, dropping out, pooling for the CNN-Model. But first, lets see what one or more convolutional layers are doing to our Simple ANN Model. Also it is necessary to understand, what dimensions we give the models to work on and what this implies (in best case: We teach the model to evaluate the heartbeat based on the \"graphical\" shape, just like a real doctor would do*\n","\n","*We use a Conv1D layer, because our Input Data is one-dimensional (i.e. is a timeseries). We have sequential and not spatial data.*\n","\n","** What can we modifiy:\n","- epochs\n","- batch_size\n","- patience\n","- Conv1_filter_num --> Numbers of watching eyes more or less?\n","- Conv1_filter_size --> test 1, 3, 5 etc --> Resolution of watching eye more or less.\n","- Pooling instead of flattening?\n","_ No. of Neurons in Dense Layers (could be also variated in Simple_ANN Model).\n","**\n","\n","**Notes: The .toarray function rounds to 4 digits, this might lead to faster, but less precise results?**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.159917Z","iopub.status.busy":"2024-03-21T08:21:41.159540Z","iopub.status.idle":"2024-03-21T08:21:41.192546Z","shell.execute_reply":"2024-03-21T08:21:41.191633Z","shell.execute_reply.started":"2024-03-21T08:21:41.159874Z"},"trusted":true},"outputs":[],"source":["if Train_Simple_CNN == True:\n","    class Config_CNN:\n","        epochs = 70 #70\n","        batch_size =10 # 10\n","        exp_name = 4\n","        patience = 70 #10\n","        initial_learning_rate=0.001 #Initial Learning Rate for ADAM: 0.001\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n","        reduce_lr_every_10_epochs = True #Reduce the learning rate each 10 Epochs with the rate defined below\n","        lr_reduction_rate = 0.5 # Reduction Rate for reducing each 10 epochs. 0.5 means, that lr is halfed each 10 epochs.\n","        Conv1_filter_num = 32\n","        Conv1_filter_size = 3\n","        filepath_checkpoint = '/kaggle/working/Simple_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.weights.h5'\n","        filepath_accuracy_plot = '/kaggle/working/Simple_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.accuracy_plot.png'\n","        filepath_loss_plot = '/kaggle/working/Simple_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.loss_plot.png'\n","        filepath_classification_report = '/kaggle/working/Simple_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.classification_report.txt'\n","\n","    cnn_model = tf.keras.models.Sequential()\n","    cnn_model.add(tf.keras.layers.Conv1D(Config_CNN.Conv1_filter_num, Config_CNN.Conv1_filter_size, activation='relu', input_shape=(187, 1))) # We add one Conv1D layer to the model\n","    cnn_model.add(tf.keras.layers.Flatten()) # After \n","    cnn_model.add(tf.keras.layers.Dense(60, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    cnn_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    cnn_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    cnn_model.add(tf.keras.layers.Dense(5, activation='softmax'))\n","    \n","    def lr_scheduler(epoch, lr):\n","        if Config_ANN.reduce_lr_every_10_epochs and epoch % 10 == 0: # %10 == 0 means each 10 epochs (only then no \"rest\" after dividing trough 10)\n","            return lr * Config_ANN.lr_reduction_rate  # reduce the learning rate with the configured rate\n","        else:\n","            return lr\n","\n","    # callback for the learning rate reduction schedule\n","    lr_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n","    \n","    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","        filepath=Config_CNN.filepath_checkpoint,\n","        save_weights_only=True,\n","        monitor='val_accuracy',\n","        mode='max',\n","        save_best_only=True)\n","\n","    early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=Config_CNN.patience)\n","\n","    cnn_model.compile(optimizer=Config_CNN.optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","    cnn_model.summary()\n","\n","    # Dfs need to be np-array\n","    #now we transform Dataframes into numpy arrays\n","    train_array = train.to_numpy()\n","    test_array = test.to_numpy()\n","    print(\"Train Dataframe shape (before reshaping):\", train.shape)\n","    print(\"Train Dataframe first 10 rows for comparison:\")\n","    #display(train.head(10)) # display(train.head(10)[0]) for first element in first row\n","    #display(train.head(10))\n","    print(\"train array shape before reshaping:\", train_array.shape)\n","    print(\"Train Array before reshaping first 10 rows for comparison:\")\n","    #display(train_array[:10]) #display(train_array[:10][0][0]) for first element in first row\n","    #display(train_array[:10])\n","\n","    # Reshape of np arrays for conv1D layer in cnn model\n","    train_reshaped = train_array.reshape(train_array.shape[0], train_array.shape[1], 1)\n","    test_reshaped = test_array.reshape(test_array.shape[0], test_array.shape[1], 1)\n","    print(\"train array shape after reshaping:\", train_reshaped.shape)\n","    print(\"Train Array after reshaping first 10 rows for comparison:\")\n","    #display(train_reshaped[:10]) #display(train_reshaped[:10][0][0]) for first element of first row\n","    \n","    # Rest of code is more or less copied from simple_ANN\n","    cnn_model_history = cnn_model.fit(train_reshaped, train_target, epochs=Config_CNN.epochs, batch_size=Config_CNN.batch_size, validation_data=(test_reshaped, test_target), callbacks=[model_checkpoint_callback, early_stop_callback, lr_scheduler_callback])\n","    #cnn_model_history = cnn_model.fit(train.reshape(train.shape[0], train.shape[1], 1), train_target, epochs=Config_CNN.epochs, batch_size=Config_CNN.batch_size, validation_data=(test.reshape(test.shape[0], test.shape[1], 1), test_target), callbacks=[model_checkpoint_callback, early_stop_callback])\n","\n","    plt.plot(cnn_model_history.history['accuracy'])\n","    plt.plot(cnn_model_history.history['val_accuracy'])\n","    plt.legend([\"accuracy\",\"val_accuracy\"])\n","    plt.title(f'Accuracy Vs Val_Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.savefig(Config_CNN.filepath_accuracy_plot)\n","    plt.show()\n","    plt.close()\n","\n","    plt.plot(cnn_model_history.history['loss'])\n","    plt.plot(cnn_model_history.history['val_loss'])\n","    plt.legend([\"loss\",\"val_loss\"])\n","    plt.title('Loss Vs Val_loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.savefig(Config_CNN.filepath_loss_plot)\n","    plt.show()\n","    plt.close()\n","\n","    predictions = cnn_model.predict(test_reshaped).argmax(axis=1)\n","\n","    report = classification_report(test_target, predictions, digits=4)\n","    print(report)\n","    with open(Config_CNN.filepath_classification_report, 'w') as report_file:\n","        report_file.write(report)\n","\n","    print(\"EVERYTHING FINISHED FOR SIMPLE CNN MODEL!\")\n","\n","else:\n","    print(\"Simple CNN Model is not trained and evaluated\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## **Advanced CNN**\n","\n","This is the Model from Hakans notebook, it will be adapted, so that the modelstructure can be used with exp1 to exp4.\n","Hakan can do some notes here ################\n","\n","Most important change(s):\n","- Sparse_Categorical_Crossentropy instead of categorical --> Should not change metrics, but removes the need for onehotencoding the dataset, which is not done in this notebook. We have numbers of 0 to 4 for each category, so its wondering, why normal categorical_cross_entropy even worked in the original notebook from hakan?\n","- By using Sparse_Categorical_Crossentropy i had to adjust the making of the classification report: test_target is now used directly and not with .argmax(axis=1). I also donÂ´t quite understand, why this should be used in any way, because test_target is already a set of predicted classes and doesnt need to be further processed? \n","-  Removed the plateu learning rate callback, we can use this in experiment5, but it was not used during exp1 to 4"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-21T08:21:41.194485Z","iopub.status.busy":"2024-03-21T08:21:41.193895Z","iopub.status.idle":"2024-03-21T08:23:07.748097Z","shell.execute_reply":"2024-03-21T08:23:07.747016Z","shell.execute_reply.started":"2024-03-21T08:21:41.194389Z"},"trusted":true},"outputs":[],"source":["if Train_Advanced_CNN == True:\n","    class Config_Advanced_CNN:\n","        epochs = 70 #70\n","        batch_size =10 # 10\n","        exp_name = 4\n","        patience = 70 #10\n","        initial_learning_rate=0.001 #Initial Learning Rate for ADAM: 0.001\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n","        reduce_lr_every_10_epochs = True #Reduce the learning rate each 10 Epochs with the rate defined below\n","        lr_reduction_rate = 0.5 # Reduction Rate for reducing each 10 epochs. 0.5 means, that lr is halfed each 10 epochs.\n","        Conv1_filter_num = 32\n","        Conv1_filter_size = 3\n","        filepath_checkpoint = '/kaggle/working/Advanced_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.weights.h5'\n","        filepath_accuracy_plot = '/kaggle/working/Advanced_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.accuracy_plot.png'\n","        filepath_loss_plot = '/kaggle/working/Advanced_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.loss_plot.png'\n","        filepath_classification_report = '/kaggle/working/Advanced_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.classification_report.txt'\n","\n","    adv_cnn_model = tf.keras.models.Sequential()\n","    adv_cnn_model.add(tf.keras.layers.Conv1D(Config_Advanced_CNN.Conv1_filter_num, Config_Advanced_CNN.Conv1_filter_size, activation='relu', input_shape=(187, 1))) # We add one Conv1D layer to the model\n","    adv_cnn_model.add(tf.keras.layers.MaxPooling1D(pool_size=3, strides=2)) # We add one Conv1D layer to the model\n","    adv_cnn_model.add(tf.keras.layers.Conv1D(Config_Advanced_CNN.Conv1_filter_num//2, Config_Advanced_CNN.Conv1_filter_size, activation='relu' )) # We add one Conv1D layer to the model\n","    adv_cnn_model.add(tf.keras.layers.MaxPooling1D(pool_size=3, strides=2)) # We add one Conv1D layer to the model\n","    adv_cnn_model.add(tf.keras.layers.Flatten()) # After  \n","    adv_cnn_model.add(tf.keras.layers.Dropout(rate=0.2))\n","    adv_cnn_model.add(tf.keras.layers.Dense(120, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    adv_cnn_model.add(tf.keras.layers.Dense(60, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    adv_cnn_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n","    adv_cnn_model.add(tf.keras.layers.Dense(5, activation='softmax'))\n","    \n","    def lr_scheduler(epoch, lr):\n","        if Config_Advanced_CNN.reduce_lr_every_10_epochs and epoch % 10 == 0: # %10 == 0 means each 10 epochs (only then no \"rest\" after dividing trough 10)\n","            return lr * Config_Advanced_CNN.lr_reduction_rate  # reduce the learning rate with the configured rate\n","        else:\n","            return lr\n","\n","    # callback for the learning rate reduction schedule\n","    lr_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n","    \n","    # Callback for the model checkpoint\n","    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","        filepath=Config_Advanced_CNN.filepath_checkpoint,\n","        save_weights_only=True,\n","        monitor='val_accuracy',\n","        mode='max',\n","        save_best_only=True)\n","    \n","    #Callback for the LR Plateu --> Not used in exp1 to exp4 and therefore commented out, but could be a really good setup for exp5 with all models?\n","    \"\"\"LR_plateu_callback = ReduceLROnPlateau(monitor = 'val_acc', min_delta = 0.001, patience = 20,\n","                                           factor = 0.5, cooldown = 0, verbose = 1)\"\"\"\n","    \n","    # Normal early stop callback\n","    early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=Config_Advanced_CNN.patience)\n","\n","    adv_cnn_model.compile(optimizer=Config_Advanced_CNN.optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","    adv_cnn_model.summary()\n","\n","    # Dfs need to be np-array\n","    #now we transform Dataframes into numpy arrays\n","    train_array = train.to_numpy()\n","    test_array = test.to_numpy()\n","    print(\"Train Dataframe shape (before reshaping):\", train.shape)\n","    print(\"Train Dataframe first 10 rows for comparison:\")\n","    #display(train.head(10)) # display(train.head(10)[0]) for first element in first row\n","    #display(train.head(10))\n","    print(\"train array shape before reshaping:\", train_array.shape)\n","    print(\"Train Array before reshaping first 10 rows for comparison:\")\n","    #display(train_array[:10]) #display(train_array[:10][0][0]) for first element in first row\n","    #display(train_array[:10])\n","\n","    # Reshape of np arrays for conv1D layer in cnn model\n","    train_reshaped = train_array.reshape(train_array.shape[0], train_array.shape[1], 1)\n","    test_reshaped = test_array.reshape(test_array.shape[0], test_array.shape[1], 1)\n","    print(\"train array shape after reshaping:\", train_reshaped.shape)\n","    print(\"Train Array after reshaping first 10 rows for comparison:\")\n","    #display(train_reshaped[:10]) #display(train_reshaped[:10][0][0]) for first element of first row\n","    \n","    # We do not use LR_plateu_callback in exp1 to exp4 and therefore remove it for now (exp5 maybe?)\n","    adv_cnn_model_history = adv_cnn_model.fit(train_reshaped, train_target, epochs=Config_Advanced_CNN.epochs, batch_size=Config_Advanced_CNN.batch_size, \n","                                      validation_data=(test_reshaped, test_target), \n","                                      callbacks=[model_checkpoint_callback, early_stop_callback, lr_scheduler_callback]) #, LR_plateu_callback\n","    \n","    plt.plot(adv_cnn_model_history.history['accuracy'])\n","    plt.plot(adv_cnn_model_history.history['val_accuracy'])\n","    plt.legend([\"accuracy\",\"val_accuracy\"])\n","    plt.title(f'Accuracy Vs Val_Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.savefig(Config_Advanced_CNN.filepath_accuracy_plot)\n","    plt.show()\n","    plt.close()\n","\n","    plt.plot(adv_cnn_model_history.history['loss'])\n","    plt.plot(adv_cnn_model_history.history['val_loss'])\n","    plt.legend([\"loss\",\"val_loss\"])\n","    plt.title('Loss Vs Val_loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.savefig(Config_Advanced_CNN.filepath_loss_plot)\n","    plt.show()\n","    plt.close()\n","\n","    predictions = adv_cnn_model.predict(test_reshaped).argmax(axis=1)\n","    \n","    ## DEBUGGING ERROR ON MAKING CLASSIFICATION REPORT\n","    print(\"predictions from adv_cnn_model:\")\n","    display(predictions)\n","    print(\"\")\n","    print(\"test target from adv_cnn_model:\") ###WHY IS TEST TARGET ALSO USED WITH argmax function? is this the same in the other codes? TGest target is NOT used with argtmax in other code.\n","    display(test_target)\n","    print(\"\")\n","    print(f\"Shape of predictions: {predictions.shape}, shape of test target: {test_target.shape} in advanced CNN Model\")\n","    \n","    report = classification_report(test_target, predictions, digits=4) # test_target used directly without .argmax(axis=1) @Hakan, why did you do this in your notebook?\n","    print(report)\n","    with open(Config_Advanced_CNN.filepath_classification_report, 'w') as report_file:\n","        report_file.write(report)\n","\n","    print(\"EVERYTHING FINISHED FOR ADVANCED CNN MODEL!\")\n","\n","else:\n","    print(\"ADVANCED CNN Model is not trained and evaluated\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":29414,"sourceId":37484,"sourceType":"datasetVersion"}],"dockerImageVersionId":30626,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0rc1"}},"nbformat":4,"nbformat_minor":4}
