{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "96a6a1e1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-08T16:37:17.976321Z",
     "iopub.status.busy": "2024-03-08T16:37:17.976012Z",
     "iopub.status.idle": "2024-03-08T16:37:20.109322Z",
     "shell.execute_reply": "2024-03-08T16:37:20.108455Z"
    },
    "papermill": {
     "duration": 2.143552,
     "end_time": "2024-03-08T16:37:20.111079",
     "exception": false,
     "start_time": "2024-03-08T16:37:17.967527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pickle\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "import xgboost as XGB\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c3a84163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/simon/.kaggle/kaggle.json'\n",
      "All Datasets are already available.\n"
     ]
    }
   ],
   "source": [
    "## Test cell for simon: Using Kaggle API to download the datasets indepent of github and its filesize limitations. Storing it in folder located outside of the repo.\n",
    "# If this works, all filepaths have to be adjusted in all notebooks to make use of the downloaded datasets.\n",
    "#RUN THIS CELL ONLY ONCE FOR ALL NOTEBOOKS!\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "#configuring and authentification with kaggle api. This could be configured so that a authentification mask is shown?\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "#Configuring the metadata for the ecg heartbeat data (original username etc)\n",
    "dataset_owner = \"shayanfazeli\"\n",
    "dataset_name = \"heartbeat\"\n",
    "\n",
    "#Configuring a download path that is NOT in the current github repo (so the big files are not pushed and cause an error!) --> Links to filepaths have to be dynamically adjusted\n",
    "download_path = \"../data/KAGGLE_datasets\" #In this case we use the data folder that is in the .gitignore list and therefore not pushed! To keep everything in one local repo.\n",
    "\n",
    "# Download structure: First check if dataset is already downloaded, else download it and store it in download path (should be outside git repo!)\n",
    "dataset_folder = os.path.join(download_path, dataset_name)\n",
    "if not os.path.exists(dataset_folder):\n",
    "    # Case 1: Dataset path is not created --> Create it and download datasets into it\n",
    "    api.dataset_download_files(dataset_owner + \"/\" + dataset_name, path=download_path + \"/\" + dataset_name, unzip=True)\n",
    "    print(\"Datasets are downloaded and unzipped.\")\n",
    "else:\n",
    "    # Case 2: Folder is created, but datasets might be missing\n",
    "    missing_files = [] \n",
    "    for file_name in [\"mitbih_test.csv\", \"mitbih_train.csv\", \"ptbdb_abnormal.csv\", \"ptbdb_normal.csv\"]:  # These are the hardcoded names of the datasets that should be downloaded.\n",
    "        file_path = os.path.join(dataset_folder, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            missing_files.append(file_name)\n",
    "\n",
    "    if missing_files:\n",
    "        # If the list contains missing files, download ALL files and overwrite the old folder.\n",
    "        api.dataset_download_files(dataset_owner + \"/\" + dataset_name, path=download_path + \"/\" + dataset_name, unzip=True, force=True)\n",
    "        print(\"Missing data was donwloaded and unzipped. All Datasets are now available.\")\n",
    "    else:\n",
    "        print(\"All Datasets are already available.\")\n",
    "\n",
    "#Creating new variable that links to the datasets and can be used in the rest of the code.\n",
    "path_to_datasets = download_path + \"/\" + dataset_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "06073fe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T16:37:20.125597Z",
     "iopub.status.busy": "2024-03-08T16:37:20.125236Z",
     "iopub.status.idle": "2024-03-08T16:37:20.128675Z",
     "shell.execute_reply": "2024-03-08T16:37:20.128113Z"
    },
    "papermill": {
     "duration": 0.01209,
     "end_time": "2024-03-08T16:37:20.130036",
     "exception": false,
     "start_time": "2024-03-08T16:37:20.117946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#only 4 digits are printed out by numpy calculations.\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "71a126b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T16:37:20.144307Z",
     "iopub.status.busy": "2024-03-08T16:37:20.143891Z",
     "iopub.status.idle": "2024-03-08T16:37:28.554329Z",
     "shell.execute_reply": "2024-03-08T16:37:28.553540Z"
    },
    "papermill": {
     "duration": 8.419779,
     "end_time": "2024-03-08T16:37:28.556365",
     "exception": false,
     "start_time": "2024-03-08T16:37:20.136586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell now makes use of the downloadfolder for the datasets. If already available locally, the filepaths can be changed.\n",
    "df_train= pd.read_csv(path_to_datasets + \"/\" + 'mitbih_train.csv', header=None)\n",
    "df_test=pd.read_csv(path_to_datasets + \"/\" +  'mitbih_test.csv',header=None)\n",
    "\n",
    "#split target and value\n",
    "train_target=df_train[187]\n",
    "test_target=df_test[187]\n",
    "train=df_train.drop(187,axis=1)\n",
    "test=df_test.drop(187,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f3cbb4eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T16:37:28.570682Z",
     "iopub.status.busy": "2024-03-08T16:37:28.570419Z",
     "iopub.status.idle": "2024-03-08T16:37:28.573932Z",
     "shell.execute_reply": "2024-03-08T16:37:28.573208Z"
    },
    "papermill": {
     "duration": 0.012332,
     "end_time": "2024-03-08T16:37:28.575526",
     "exception": false,
     "start_time": "2024-03-08T16:37:28.563194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Switches for the user to define which sample method is used and which models are run.\n",
    "class Config:\n",
    "    oversample = False #refers to mitbih B_SMOTE\n",
    "    undersample = False #Refers to undersampling with random undersampler\n",
    "    sample_name = \"UNDEFINED_SAMPLE\"\n",
    "\n",
    "Train_SVM =  False #trains the SVM Model without Gridsearch\n",
    "Train_KNN = False #trains the KNN Model without Gridsearch\n",
    "Train_DTC = False #trains the DTC Model without Gridsearch\n",
    "Train_RF = False #trains the RF Model without Gridsearch\n",
    "Train_XGB = False #trains the XGB Model without Gridsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "045ff622",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T16:37:28.590061Z",
     "iopub.status.busy": "2024-03-08T16:37:28.589796Z",
     "iopub.status.idle": "2024-03-08T16:37:28.593402Z",
     "shell.execute_reply": "2024-03-08T16:37:28.592581Z"
    },
    "papermill": {
     "duration": 0.012892,
     "end_time": "2024-03-08T16:37:28.595250",
     "exception": false,
     "start_time": "2024-03-08T16:37:28.582358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "oversampler = SMOTE()\n",
    "undersampler = RandomUnderSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "303ceae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T16:37:28.609288Z",
     "iopub.status.busy": "2024-03-08T16:37:28.609005Z",
     "iopub.status.idle": "2024-03-08T16:37:29.972590Z",
     "shell.execute_reply": "2024-03-08T16:37:29.971888Z"
    },
    "papermill": {
     "duration": 1.372662,
     "end_time": "2024-03-08T16:37:29.974472",
     "exception": false,
     "start_time": "2024-03-08T16:37:28.601810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the original mitbih dataset\n",
      "Sample Name: MITBIH_A_Original\n"
     ]
    }
   ],
   "source": [
    "#Based on the user settings, Resampling is done and the sample name (i.e. filenames) are modified.\n",
    "if Config.oversample:\n",
    "    train, train_target = oversampler.fit_resample(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n",
    "    Config.sample_name = \"MITBIH_B_SMOTE\"\n",
    "    print(\"Sample Name:\", Config.sample_name)\n",
    "elif Config.undersample:\n",
    "    train, train_target = undersampler.fit_resample(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n",
    "    Config.sample_name = \"MITBIH_C_RUS\"\n",
    "    print(\"Sample Name:\", Config.sample_name)\n",
    "else: \n",
    "    print(\"Using the original mitbih dataset\")\n",
    "    Config.sample_name = \"MITBIH_A_Original\"\n",
    "    print(\"Sample Name:\", Config.sample_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "12e9d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to save models and classification report directly after running.\n",
    "def save_model_and_report(model, report, model_filename, report_filename, model_folder=\"../models/ML_Models\", report_folder=\"../reports/figures/ML_Models\"):\n",
    "    # Save the model\n",
    "    model_savepath = os.path.join(model_folder, model_filename)\n",
    "    with open(model_savepath, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"The model was saved as {model_filename} in folder {model_folder}.\")\n",
    "\n",
    "    # Check if model file size is greater than 98MB (Restriction for github only!)\n",
    "    if os.path.getsize(model_savepath) > 98 * 1024 * 1024:  # Check if size is greater than 98MB\n",
    "        print(\"Model file size is too big. Changing save path...\")\n",
    "        model_folder = \"../data/models_too_big_for_git\"\n",
    "        os.makedirs(model_folder, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "        new_model_savepath = os.path.join(model_folder, model_filename)\n",
    "        os.replace(model_savepath, new_model_savepath)  # Move the model to the new location\n",
    "        print(f\"Model moved to {model_folder} due to its size.\")\n",
    "\n",
    "    # Save the classification report\n",
    "    report_savepath = os.path.join(report_folder, report_filename)\n",
    "    with open(report_savepath, \"w\") as f:\n",
    "        f.write(report)\n",
    "    print(f\"The classification report was saved as {report_filename} in folder {report_folder}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "da81ef52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T16:37:29.989075Z",
     "iopub.status.busy": "2024-03-08T16:37:29.988820Z",
     "iopub.status.idle": "2024-03-08T16:37:29.994384Z",
     "shell.execute_reply": "2024-03-08T16:37:29.993649Z"
    },
    "papermill": {
     "duration": 0.014706,
     "end_time": "2024-03-08T16:37:29.996128",
     "exception": false,
     "start_time": "2024-03-08T16:37:29.981422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87554, 187)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b9fe67",
   "metadata": {
    "papermill": {
     "duration": 0.006442,
     "end_time": "2024-03-08T16:37:30.009438",
     "exception": false,
     "start_time": "2024-03-08T16:37:30.002996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **SVM**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "140181b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T16:37:30.024672Z",
     "iopub.status.busy": "2024-03-08T16:37:30.024196Z",
     "iopub.status.idle": "2024-03-08T16:37:30.027938Z",
     "shell.execute_reply": "2024-03-08T16:37:30.027182Z"
    },
    "papermill": {
     "duration": 0.013468,
     "end_time": "2024-03-08T16:37:30.029503",
     "exception": false,
     "start_time": "2024-03-08T16:37:30.016035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model withoud gridsearch is not trained\n"
     ]
    }
   ],
   "source": [
    "#Just the code for model creation, fitting and creating the report out of the predictions.\n",
    "if Train_SVM == True:\n",
    "    model = SVC(cache_size=500)\n",
    "    model.fit(train,train_target)\n",
    "    predictions = model.predict(test)\n",
    "    report=classification_report(test_target, predictions, digits=4)\n",
    "    print(report)\n",
    "    #Calling the save_model_and_report function for the SVM Model\n",
    "    model_filename = f\"SVM_Basemodel_no_gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"SVM_Basemodel_no_gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    save_model_and_report(model, report, model_filename, report_filename)\n",
    "else:\n",
    "    print(\"SVM Model withoud gridsearch is not trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66785f33",
   "metadata": {
    "papermill": {
     "duration": 0.006836,
     "end_time": "2024-03-08T17:18:35.863195",
     "exception": false,
     "start_time": "2024-03-08T17:18:35.856359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **KNN**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3784bfce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T17:18:35.878594Z",
     "iopub.status.busy": "2024-03-08T17:18:35.878298Z",
     "iopub.status.idle": "2024-03-08T17:18:35.881911Z",
     "shell.execute_reply": "2024-03-08T17:18:35.881200Z"
    },
    "papermill": {
     "duration": 0.013146,
     "end_time": "2024-03-08T17:18:35.883733",
     "exception": false,
     "start_time": "2024-03-08T17:18:35.870587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Model without gridsearch is not trained.\n"
     ]
    }
   ],
   "source": [
    "if Train_KNN == True:\n",
    "    model = KNN(n_jobs = -1)\n",
    "    model.fit(train,train_target)\n",
    "    predictions = model.predict(test)\n",
    "    report=classification_report(test_target, predictions, digits=4)\n",
    "    print(report)\n",
    "    #Calling the save_model_and_report function for the KNN Model\n",
    "    model_filename = f\"KNN_Basemodel_no_gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"KNN_Basemodel_no_gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    save_model_and_report(model, report, model_filename, report_filename)\n",
    "else:\n",
    "    print(\"KNN Model without gridsearch is not trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278d4c36",
   "metadata": {
    "papermill": {
     "duration": 0.006902,
     "end_time": "2024-03-08T17:30:16.867283",
     "exception": false,
     "start_time": "2024-03-08T17:30:16.860381",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Decision Tree**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9ba3453f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T17:30:16.883819Z",
     "iopub.status.busy": "2024-03-08T17:30:16.883518Z",
     "iopub.status.idle": "2024-03-08T17:30:16.886952Z",
     "shell.execute_reply": "2024-03-08T17:30:16.886210Z"
    },
    "papermill": {
     "duration": 0.01312,
     "end_time": "2024-03-08T17:30:16.888499",
     "exception": false,
     "start_time": "2024-03-08T17:30:16.875379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Model without gridsearch is not trained.\n"
     ]
    }
   ],
   "source": [
    "if Train_DTC == True:\n",
    "    model = DTC()\n",
    "    model.fit(train,train_target)\n",
    "    predictions = model.predict(test)\n",
    "    report=classification_report(test_target, predictions, digits=4)\n",
    "    print(report)\n",
    "    #Calling the save_model_and_report function for the Decision Tree Model\n",
    "    model_filename = f\"DTC_Basemodel_no_gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"DTC_Basemodel_no_gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    save_model_and_report(model, report, model_filename, report_filename)\n",
    "else:\n",
    "    print(\"Decision Tree Model without gridsearch is not trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a899a01",
   "metadata": {
    "papermill": {
     "duration": 0.007861,
     "end_time": "2024-03-08T17:34:11.478217",
     "exception": false,
     "start_time": "2024-03-08T17:34:11.470356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Random Forest**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "37e1a610",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T17:34:11.495348Z",
     "iopub.status.busy": "2024-03-08T17:34:11.494517Z",
     "iopub.status.idle": "2024-03-08T17:34:11.498346Z",
     "shell.execute_reply": "2024-03-08T17:34:11.497634Z"
    },
    "papermill": {
     "duration": 0.014147,
     "end_time": "2024-03-08T17:34:11.500026",
     "exception": false,
     "start_time": "2024-03-08T17:34:11.485879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model without gridsearch is not trained.\n"
     ]
    }
   ],
   "source": [
    "if Train_RF == True:\n",
    "    model = RFC(n_jobs = -1)\n",
    "    model.fit(train,train_target)\n",
    "    predictions = model.predict(test)\n",
    "    report=classification_report(test_target, predictions, digits=4)\n",
    "    print(report)\n",
    "    #Calling the save_model_and_report function for the Random Forest Model\n",
    "    model_filename = f\"RFC_Basemodel_no_gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"RFC_Basemodel_no_gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    save_model_and_report(model, report, model_filename, report_filename)\n",
    "else:\n",
    "    print(\"Random Forest Model without gridsearch is not trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ef92a",
   "metadata": {
    "papermill": {
     "duration": 0.007353,
     "end_time": "2024-03-08T17:37:53.962258",
     "exception": false,
     "start_time": "2024-03-08T17:37:53.954905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **XGBoost**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5a866d4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T17:37:53.978948Z",
     "iopub.status.busy": "2024-03-08T17:37:53.978443Z",
     "iopub.status.idle": "2024-03-08T17:37:53.981708Z",
     "shell.execute_reply": "2024-03-08T17:37:53.981176Z"
    },
    "papermill": {
     "duration": 0.013253,
     "end_time": "2024-03-08T17:37:53.983124",
     "exception": false,
     "start_time": "2024-03-08T17:37:53.969871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XBG Model without gridsearch is not trained.\n"
     ]
    }
   ],
   "source": [
    "if Train_XGB == True:\n",
    "    model = XGB.XGBClassifier(objective='binary:logistic')\n",
    "    model.fit(train,train_target)\n",
    "    predictions = model.predict(test)\n",
    "    report=classification_report(test_target, predictions, digits=4)\n",
    "    print(report)\n",
    "    #Calling the save_model_and_report function for the XGB Model\n",
    "    model_filename = f\"XGB_Basemodel_no_gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"XGB_Basemodel_no_gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    save_model_and_report(model, report, model_filename, report_filename)\n",
    "else:\n",
    "    print(\"XBG Model without gridsearch is not trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c9e818",
   "metadata": {},
   "source": [
    "# Gridsearch Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d52f90",
   "metadata": {},
   "source": [
    "We present in the following section the gridsearches, that each team member did. Therefore, the specific codes are presented. If possible,\n",
    "the codes are rewritten to be more compact and fit the overall style of this final notebook. Also the filesaving code lines are added. Other than that, the original code is reused.\n",
    "\n",
    "If files from the gridsearch were pushed to git, the following must be kept in mind:\n",
    "- Dataset C is the smallest and was used for debugging the code, but not used in the reports\n",
    "- Dataset B is way too big to be run on a single computer. If needed, the provided code can be run on Kaggle with some changes to the filepaths. Since Dataset B Gridsearch was also not included in the report, we do not provide the files. The code can generate the gridsearch results for Dataset B if needed.\n",
    "- Dataset A was redone and pushed to github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f9de0763",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary imports\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import neighbors\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f27d79c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Dataset that is used for the gridsearch: MITBIH_A_Original\n"
     ]
    }
   ],
   "source": [
    "# Configuration switches / Paramgrids\n",
    "Gridsearch_SVM = False # (Done by Hakan?)\n",
    "Gridsearch_KNN = False # (Done by Alex)\n",
    "Gridsearch_DTC = False # (Done by Alex)\n",
    "Gridsearch_RFC = False # (Done by Simon)\n",
    "Gridsearch_XGB = False # (Done by Hakan)\n",
    "\n",
    "#print the current dataset that is in usage:\n",
    "print(\"Current Dataset that is used for the gridsearch:\", Config.sample_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed41abc",
   "metadata": {},
   "source": [
    "## Gridsearch SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3a797309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Gridsearch for SVM is performed.\n"
     ]
    }
   ],
   "source": [
    "if Gridsearch_SVM == True:\n",
    "    svm_model = SVC()\n",
    "    params = {'C': [1, 10, 100], 'kernel': [ 'linear','rbf']}\n",
    "    grid_search = GridSearchCV(estimator=svm_model, param_grid=params, verbose=3, cv=5, n_jobs=-1)\n",
    "    grid_search.fit(train, train_target)\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    y_pred = grid_search.predict(test)\n",
    "    print(pd.crosstab(test_target, y_pred, rownames=['Class Actual'], colnames=['Class Predict']))\n",
    "    print(classification_report(test_target, y_pred))\n",
    "\n",
    "    print(\"Accuracy Score on Test Set: \", grid_search.score(test, test_target))\n",
    "    model_filename = f\"SVM_Optimized_Model_with_Gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"SVM_Optimized_Model_with_Gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    # Saving the best model and the associated classification report\n",
    "    save_model_and_report(grid_search.best_estimator_, classification_report(test_target, y_pred), model_filename, report_filename)\n",
    "\n",
    "else:\n",
    "    print(\"No Gridsearch for SVM is performed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b1794e",
   "metadata": {},
   "source": [
    "## Gridsearch KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "56378768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Gridsearch for KNN is performed.\n"
     ]
    }
   ],
   "source": [
    "if Gridsearch_KNN == True:\n",
    "    knn = neighbors.KNeighborsClassifier(n_jobs=-1)\n",
    "    param_knn = {'metric': ['manhattan'],'n_neighbors': [1]} #,'minkowski','chebyshev','euclidean' // k for  k in range(1, 7)\n",
    "    grid=GridSearchCV(estimator=knn,param_grid=param_knn,verbose=3, cv=5 , scoring='accuracy', n_jobs=-1)\n",
    "    grid.fit(train,train_target)\n",
    "    print(pd.DataFrame(grid.cv_results_)[['params', 'mean_test_score', 'std_test_score']])\n",
    "\n",
    "    print('Best Metric:', grid.best_estimator_.get_params()['metric'])\n",
    "    print('Best K:', grid.best_estimator_.get_params()['n_neighbors'])\n",
    "\n",
    "    y_pred=grid.predict(test)\n",
    "    print(pd.crosstab(test_target,y_pred,rownames=['Class Actual'],colnames=['Class Predict']))\n",
    "    print(classification_report(test_target,y_pred))\n",
    "\n",
    "\n",
    "    print(\"Accuracy Score on Test Set: \", grid.score(test, test_target))\n",
    "    model_filename = f\"KNN_Optimized_Model_with_Gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"KNN_Optimized_Model_with_Gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    #saving the best model and the associated classification report\n",
    "    save_model_and_report(grid.best_estimator_, classification_report(test_target, y_pred), model_filename, report_filename)\n",
    "else:\n",
    "    print(\"No Gridsearch for KNN is performed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e52220",
   "metadata": {},
   "source": [
    "## Gridsearch DTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d6bbea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Gridsearch for DTC is performed.\n"
     ]
    }
   ],
   "source": [
    "if Gridsearch_DTC == True:\n",
    "    dt=DecisionTreeClassifier()\n",
    "    param_dt={'criterion': ['entropy'],   'max_depth': [12]} # 'gini', // 2,4,6,8,10,\n",
    "    grid=GridSearchCV(estimator=dt,param_grid=param_dt, verbose=3, cv=5 , scoring='accuracy', n_jobs=-1)\n",
    "    grid.fit(train, train_target)\n",
    "    pd.DataFrame(grid.cv_results_)[['params', 'mean_test_score', 'std_test_score']]\n",
    "\n",
    "    print('Best Metric:', grid.best_estimator_.get_params()['criterion'])\n",
    "    print('Best Depth:', grid.best_estimator_.get_params()['max_depth'])\n",
    "\n",
    "    y_pred=grid.predict(test)\n",
    "    print(pd.crosstab(test_target,y_pred,rownames=['Class Actual'],colnames=['Class Predict']))\n",
    "    print(classification_report(test_target,y_pred))\n",
    "\n",
    "    print(\"Accuracy Score on Test Set: \", grid.score(test, test_target))\n",
    "    model_filename = f\"DTC_Optimized_Model_with_Gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"DTC_Optimized_Model_with_Gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    #saving the best model and the associated classification report\n",
    "    save_model_and_report(grid.best_estimator_, classification_report(test_target, y_pred), model_filename, report_filename)\n",
    "else:\n",
    "    print(\"No Gridsearch for DTC is performed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a56185",
   "metadata": {},
   "source": [
    "## Gridsearch RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b6fc6c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Gridsearch for RFC is performed.\n"
     ]
    }
   ],
   "source": [
    "if Gridsearch_RFC == True:\n",
    "    param_grid = {\n",
    "        'n_estimators': [200],  #50, 100, \n",
    "        'criterion': ['entropy'], #'gini',\n",
    "        'max_depth': [None], #, 10, 20\n",
    "        'min_samples_split': [2], #, 5 \n",
    "        'min_samples_leaf': [1], #, 2, 4 \n",
    "        'max_features': ['sqrt'] #, 'log2'    \n",
    "    }\n",
    "    rfc_grid = RandomForestClassifier(n_jobs=-1)\n",
    "    grid_search = GridSearchCV(estimator = rfc_grid, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs = -1, verbose=3)\n",
    "    grid_search.fit(train, train_target)\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    y_pred=grid_search.predict(test)\n",
    "    print(pd.crosstab(test_target,y_pred,rownames=['Class Actual'],colnames=['Class Predict']))\n",
    "    print(classification_report(test_target,y_pred))\n",
    "\n",
    "    print(\"Accuracy Score on Test Set: \", grid_search.score(test, test_target))\n",
    "    model_filename = f\"RFC_Optimized_Model_with_Gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"RFC_Optimized_Model_with_Gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    #saving the best model and the associated classification report\n",
    "    save_model_and_report(grid_search.best_estimator_, classification_report(test_target, y_pred), model_filename, report_filename)\n",
    "\n",
    "else:\n",
    "    print(\"No Gridsearch for RFC is performed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad9aee2",
   "metadata": {},
   "source": [
    "## Gridsearch XGB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "93f5e1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Gridsearch for XBG is performed.\n"
     ]
    }
   ],
   "source": [
    "if Gridsearch_XGB == True:\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',  # for binary classification\n",
    "        n_jobs=-1,\n",
    "        #max_depth=10,\n",
    "        #learning_rate=0.01,\n",
    "        #n_estimators=100\n",
    "    )\n",
    "    params={'max_depth':[50], 'n_estimators':[2000], 'learning_rate': [0.2]} #1500, 2000, 2500 // 0.1, 0.2, 0.5\n",
    "    grid_search = GridSearchCV(estimator=xgb_model,param_grid=params, verbose=3, cv=5, n_jobs=-1)\n",
    "    grid_search.fit(train, train_target)\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    y_pred=grid_search.predict(test)\n",
    "    print(pd.crosstab(test_target,y_pred,rownames=['Class Actual'],colnames=['Class Predict']))\n",
    "    print(classification_report(test_target,y_pred))\n",
    "\n",
    "    print(\"Accuracy Score on Test Set: \", grid_search.score(test, test_target))\n",
    "    model_filename = f\"XGB_Optimized_Model_with_Gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"XGB_Optimized_Model_with_Gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    #saving the best model and the associated classification report\n",
    "    save_model_and_report(grid_search.best_estimator_, classification_report(test_target, y_pred), model_filename, report_filename)\n",
    "\n",
    "else:\n",
    "    print(\"No Gridsearch for XBG is performed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 29414,
     "sourceId": 37484,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3755.277893,
   "end_time": "2024-03-08T17:39:50.297430",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-08T16:37:15.019537",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
