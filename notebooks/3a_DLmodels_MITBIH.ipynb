{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":37484,"sourceType":"datasetVersion","datasetId":29414}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra \nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import accuracy_score, classification_report\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nif tf.config.experimental.list_physical_devices('GPU'):\n    print('GPU is used')\nelse:\n    print('no GPU available, CPU is used instead')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-21T08:21:29.078141Z","iopub.execute_input":"2024-03-21T08:21:29.078990Z","iopub.status.idle":"2024-03-21T08:21:29.100221Z","shell.execute_reply.started":"2024-03-21T08:21:29.078956Z","shell.execute_reply":"2024-03-21T08:21:29.099372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.set_printoptions(precision=4)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T08:21:29.101733Z","iopub.execute_input":"2024-03-21T08:21:29.102011Z","iopub.status.idle":"2024-03-21T08:21:29.106371Z","shell.execute_reply.started":"2024-03-21T08:21:29.101988Z","shell.execute_reply":"2024-03-21T08:21:29.105285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train= pd.read_csv('/kaggle/input/heartbeat/mitbih_train.csv', header=None)\ndf_test=pd.read_csv('/kaggle/input/heartbeat/mitbih_test.csv',header=None)\nprint(\"Dataframes MITBIH correctly read into workspace\")\n\n#split target and value\ntrain_target=df_train[187]\ntest_target=df_test[187]\ntrain=df_train.drop(187,axis=1)\ntest=df_test.drop(187,axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T08:21:29.107375Z","iopub.execute_input":"2024-03-21T08:21:29.107643Z","iopub.status.idle":"2024-03-21T08:21:41.062102Z","shell.execute_reply.started":"2024-03-21T08:21:29.107621Z","shell.execute_reply":"2024-03-21T08:21:41.060979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config_Sampling:\n    oversample = True #equals to B_SMOTE\n    undersample = False\n    sample_name = \"UNDEFINED_SAMPLE\"\n    \n    \n\nTrain_Simple_ANN = False #Trains the simple ANN\nTrain_Simple_CNN = False #Trains the simple CNN\nTrain_Advanced_CNN = True #Trains the advanced CNN\nTrain_Simple_RNN = False #Trains the simple RNN\n# TRAIN OTHER MODELS ## Placeholder for switch for other models\n ","metadata":{"execution":{"iopub.status.busy":"2024-03-21T08:21:41.065436Z","iopub.execute_input":"2024-03-21T08:21:41.065807Z","iopub.status.idle":"2024-03-21T08:21:41.072551Z","shell.execute_reply.started":"2024-03-21T08:21:41.065770Z","shell.execute_reply":"2024-03-21T08:21:41.071545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oversampler = SMOTE()\nundersampler = RandomUnderSampler()","metadata":{"execution":{"iopub.status.busy":"2024-03-21T08:21:41.074106Z","iopub.execute_input":"2024-03-21T08:21:41.074675Z","iopub.status.idle":"2024-03-21T08:21:41.088102Z","shell.execute_reply.started":"2024-03-21T08:21:41.074641Z","shell.execute_reply":"2024-03-21T08:21:41.087129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if Config_Sampling.oversample:\n    train, train_target = oversampler.fit_resample(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n    Config_Sampling.sample_name = \"MITBIH_B_SMOTE\"\n    print(\"Sample Name:\", Config_Sampling.sample_name)\nelif Config_Sampling.undersample:\n    train, train_target = undersampler.fit_resample(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n    Config_Sampling.sample_name = \"MITBIH_C_RUS\"\n    print(\"Sample Name:\", Config_Sampling.sample_name)\nelse: \n    print(\"Using the original mitbih dataset\")\n    Config_Sampling.sample_name = \"MITBIH_A_Original\"\n    print(\"Sample Name:\", Config_Sampling.sample_name)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T08:21:41.089546Z","iopub.execute_input":"2024-03-21T08:21:41.089884Z","iopub.status.idle":"2024-03-21T08:21:41.099952Z","shell.execute_reply.started":"2024-03-21T08:21:41.089853Z","shell.execute_reply":"2024-03-21T08:21:41.099018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#overview for later comparison (has the model produced the right output?)\nprint(\"Initial Train shape\", train.shape)\nprint(\"Initial Test shape\", test.shape)\nprint(\"Initial Value count for train dataset (target):\", train_target.value_counts())\nprint(\"Initial Value count for test dataset (target):\", test_target.value_counts())\n\nprint(\"Initial percentages for train dataset (target):\", 100* train_target.value_counts() / len(train_target))\nprint(\"Initial percentages for test dataset (target):\", 100 *test_target.value_counts() / len(test_target))","metadata":{"execution":{"iopub.status.busy":"2024-03-21T08:21:41.101950Z","iopub.execute_input":"2024-03-21T08:21:41.102201Z","iopub.status.idle":"2024-03-21T08:21:41.129922Z","shell.execute_reply.started":"2024-03-21T08:21:41.102179Z","shell.execute_reply":"2024-03-21T08:21:41.128825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Deep Learning Models**","metadata":{}},{"cell_type":"markdown","source":"## **Simple Artificial neural Network**\nANN without convolutional layers. Only Dense layers are used. No Pooling, Flattening or Dropping out. Base model for later comparison.","metadata":{}},{"cell_type":"code","source":"if Train_Simple_ANN == True:\n    class Config_ANN:\n        epochs = 70 #70\n        batch_size = 10 #10\n        exp_name = 4 #Please WRITE THIS DOWN IN EXCEL FILE (With screenshot)\n        patience = 70 #10 # for early stopping\n        initial_learning_rate=0.001 #Initial Learning Rate for ADAM: 0.001\n        optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n        reduce_lr_every_10_epochs = True #Reduce the learning rate each 10 Epochs with the rate defined below\n        lr_reduction_rate = 0.5 # Reduction Rate for reducing each 10 epochs. 0.5 means, that lr is halfed each 10 epochs.\n        filepath_checkpoint = '/kaggle/working/Simple_ANN/'+str(Config_Sampling.sample_name) +'/experiment_'+str(exp_name) + '_'+str (Config_Sampling.sample_name) +'.weights.h5'\n        filepath_accuracy_plot = '/kaggle/working/Simple_ANN/'+str(Config_Sampling.sample_name) +'/experiment_'+str(exp_name) +  '_'+ str (Config_Sampling.sample_name) +'.accuracy_plot.png'\n        filepath_loss_plot = '/kaggle/working/Simple_ANN/'+str(Config_Sampling.sample_name) +'/experiment_'+str(exp_name) +  '_'+str (Config_Sampling.sample_name) +'.loss_plot.png'\n        filepath_classification_report = '/kaggle/working/Simple_ANN/'+str(Config_Sampling.sample_name) +'/experiment_'+str(exp_name) +  '_'+str (Config_Sampling.sample_name) +'.classification_report.txt'\n\n    ann_model = tf.keras.models.Sequential()\n    ann_model.add(tf.keras.layers.Dense(60, activation=tf.keras.layers.LeakyReLU(alpha=0.001), input_shape=(187,)))\n    ann_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n    ann_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n    ann_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n    ann_model.add(tf.keras.layers.Dense(5, activation='softmax'))\n\n    def lr_scheduler(epoch, lr):\n        if Config_ANN.reduce_lr_every_10_epochs and epoch % 10 == 0: # %10 == 0 means each 10 epochs (only then no \"rest\" after dividing trough 10)\n            return lr * Config_ANN.lr_reduction_rate  # reduce the learning rate with the configured rate\n        else:\n            return lr\n\n    # callback for the learning rate reduction schedule\n    lr_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n\n    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n        filepath=Config_ANN.filepath_checkpoint,\n        save_weights_only=True,\n        monitor='val_accuracy',\n        mode='max',\n        save_best_only=True)\n    \n    early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=Config_ANN.patience)\n\n    ann_model.compile(optimizer=Config_ANN.optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    ann_model.summary()\n\n    ann_model_history = ann_model.fit(train, train_target, epochs=Config_ANN.epochs, batch_size = Config_ANN.batch_size, validation_data = (test, test_target), callbacks=[model_checkpoint_callback, early_stop_callback, lr_scheduler_callback])\n\n    plt.plot(ann_model_history.history['accuracy'])\n    plt.plot(ann_model_history.history['val_accuracy'])\n    plt.legend([\"accuracy\",\"val_accuracy\"])\n    plt.title('Accuracy Vs Val_Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.savefig(Config_ANN.filepath_accuracy_plot)\n    plt.show()\n    plt.close()\n\n\n    plt.plot(ann_model_history.history['loss'])\n    plt.plot(ann_model_history.history['val_loss'])\n    plt.legend([\"loss\",\"val_loss\"])\n    plt.title('Loss Vs Val_loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.savefig(Config_ANN.filepath_loss_plot)\n    plt.show()\n    plt.close()\n\n    predictions = ann_model.predict(test).argmax(axis=1) #directly getting classes instead of probabilities.\n\n    report=classification_report(test_target, predictions, digits=4)\n    print(report)\n    with open(Config_ANN.filepath_classification_report, 'w') as report_file:\n        report_file.write(report)\n\n    print(\"EVERYTHING FINISHED FOR SIMPLE ANN MODEL!\")\n\nelse:\n    print(\"Simple ANN Model is not trained and evaluated\")\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-21T08:21:41.133136Z","iopub.execute_input":"2024-03-21T08:21:41.133496Z","iopub.status.idle":"2024-03-21T08:21:41.158193Z","shell.execute_reply.started":"2024-03-21T08:21:41.133465Z","shell.execute_reply":"2024-03-21T08:21:41.157083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Simple Convolutional neural Network**\nCNN with convolutional layers. The same Dense layers from Simple ANN Model are used. No Pooling, Flattening or Dropping out. Base model for later comparison.\n\n*Notes: We could try different variations of flattening, dropping out, pooling for the CNN-Model. But first, lets see what one or more convolutional layers are doing to our Simple ANN Model. Also it is necessary to understand, what dimensions we give the models to work on and what this implies (in best case: We teach the model to evaluate the heartbeat based on the \"graphical\" shape, just like a real doctor would do*\n\n*We use a Conv1D layer, because our Input Data is one-dimensional (i.e. is a timeseries). We have sequential and not spatial data.*\n\n** What can we modifiy:\n- epochs\n- batch_size\n- patience\n- Conv1_filter_num --> Numbers of watching eyes more or less?\n- Conv1_filter_size --> test 1, 3, 5 etc --> Resolution of watching eye more or less.\n- Pooling instead of flattening?\n_ No. of Neurons in Dense Layers (could be also variated in Simple_ANN Model).\n**\n\n**Notes: The .toarray function rounds to 4 digits, this might lead to faster, but less precise results?**\n","metadata":{}},{"cell_type":"code","source":"if Train_Simple_CNN == True:\n    class Config_CNN:\n        epochs = 70 #70\n        batch_size =10 # 10\n        exp_name = 4\n        patience = 70 #10\n        initial_learning_rate=0.001 #Initial Learning Rate for ADAM: 0.001\n        optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n        reduce_lr_every_10_epochs = True #Reduce the learning rate each 10 Epochs with the rate defined below\n        lr_reduction_rate = 0.5 # Reduction Rate for reducing each 10 epochs. 0.5 means, that lr is halfed each 10 epochs.\n        Conv1_filter_num = 32\n        Conv1_filter_size = 3\n        filepath_checkpoint = '/kaggle/working/Simple_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.weights.h5'\n        filepath_accuracy_plot = '/kaggle/working/Simple_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.accuracy_plot.png'\n        filepath_loss_plot = '/kaggle/working/Simple_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.loss_plot.png'\n        filepath_classification_report = '/kaggle/working/Simple_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.classification_report.txt'\n\n    cnn_model = tf.keras.models.Sequential()\n    cnn_model.add(tf.keras.layers.Conv1D(Config_CNN.Conv1_filter_num, Config_CNN.Conv1_filter_size, activation='relu', input_shape=(187, 1))) # We add one Conv1D layer to the model\n    cnn_model.add(tf.keras.layers.Flatten()) # After \n    cnn_model.add(tf.keras.layers.Dense(60, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n    cnn_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n    cnn_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n    cnn_model.add(tf.keras.layers.Dense(5, activation='softmax'))\n    \n    def lr_scheduler(epoch, lr):\n        if Config_ANN.reduce_lr_every_10_epochs and epoch % 10 == 0: # %10 == 0 means each 10 epochs (only then no \"rest\" after dividing trough 10)\n            return lr * Config_ANN.lr_reduction_rate  # reduce the learning rate with the configured rate\n        else:\n            return lr\n\n    # callback for the learning rate reduction schedule\n    lr_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n    \n    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n        filepath=Config_CNN.filepath_checkpoint,\n        save_weights_only=True,\n        monitor='val_accuracy',\n        mode='max',\n        save_best_only=True)\n\n    early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=Config_CNN.patience)\n\n    cnn_model.compile(optimizer=Config_CNN.optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    cnn_model.summary()\n\n    # Dfs need to be np-array\n    #now we transform Dataframes into numpy arrays\n    train_array = train.to_numpy()\n    test_array = test.to_numpy()\n    print(\"Train Dataframe shape (before reshaping):\", train.shape)\n    print(\"Train Dataframe first 10 rows for comparison:\")\n    #display(train.head(10)) # display(train.head(10)[0]) for first element in first row\n    #display(train.head(10))\n    print(\"train array shape before reshaping:\", train_array.shape)\n    print(\"Train Array before reshaping first 10 rows for comparison:\")\n    #display(train_array[:10]) #display(train_array[:10][0][0]) for first element in first row\n    #display(train_array[:10])\n\n    # Reshape of np arrays for conv1D layer in cnn model\n    train_reshaped = train_array.reshape(train_array.shape[0], train_array.shape[1], 1)\n    test_reshaped = test_array.reshape(test_array.shape[0], test_array.shape[1], 1)\n    print(\"train array shape after reshaping:\", train_reshaped.shape)\n    print(\"Train Array after reshaping first 10 rows for comparison:\")\n    #display(train_reshaped[:10]) #display(train_reshaped[:10][0][0]) for first element of first row\n    \n    # Rest of code is more or less copied from simple_ANN\n    cnn_model_history = cnn_model.fit(train_reshaped, train_target, epochs=Config_CNN.epochs, batch_size=Config_CNN.batch_size, validation_data=(test_reshaped, test_target), callbacks=[model_checkpoint_callback, early_stop_callback, lr_scheduler_callback])\n    #cnn_model_history = cnn_model.fit(train.reshape(train.shape[0], train.shape[1], 1), train_target, epochs=Config_CNN.epochs, batch_size=Config_CNN.batch_size, validation_data=(test.reshape(test.shape[0], test.shape[1], 1), test_target), callbacks=[model_checkpoint_callback, early_stop_callback])\n\n    plt.plot(cnn_model_history.history['accuracy'])\n    plt.plot(cnn_model_history.history['val_accuracy'])\n    plt.legend([\"accuracy\",\"val_accuracy\"])\n    plt.title(f'Accuracy Vs Val_Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.savefig(Config_CNN.filepath_accuracy_plot)\n    plt.show()\n    plt.close()\n\n    plt.plot(cnn_model_history.history['loss'])\n    plt.plot(cnn_model_history.history['val_loss'])\n    plt.legend([\"loss\",\"val_loss\"])\n    plt.title('Loss Vs Val_loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.savefig(Config_CNN.filepath_loss_plot)\n    plt.show()\n    plt.close()\n\n    predictions = cnn_model.predict(test_reshaped).argmax(axis=1)\n\n    report = classification_report(test_target, predictions, digits=4)\n    print(report)\n    with open(Config_CNN.filepath_classification_report, 'w') as report_file:\n        report_file.write(report)\n\n    print(\"EVERYTHING FINISHED FOR SIMPLE CNN MODEL!\")\n\nelse:\n    print(\"Simple CNN Model is not trained and evaluated\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T08:21:41.159540Z","iopub.execute_input":"2024-03-21T08:21:41.159917Z","iopub.status.idle":"2024-03-21T08:21:41.192546Z","shell.execute_reply.started":"2024-03-21T08:21:41.159874Z","shell.execute_reply":"2024-03-21T08:21:41.191633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Advanced CNN**\n\nThis is the Model from Hakans notebook, it will be adapted, so that the modelstructure can be used with exp1 to exp4.\nHakan can do some notes here ################\n\nMost important change(s):\n- Sparse_Categorical_Crossentropy instead of categorical --> Should not change metrics, but removes the need for onehotencoding the dataset, which is not done in this notebook. We have numbers of 0 to 4 for each category, so its wondering, why normal categorical_cross_entropy even worked in the original notebook from hakan?\n- By using Sparse_Categorical_Crossentropy i had to adjust the making of the classification report: test_target is now used directly and not with .argmax(axis=1). I also donÂ´t quite understand, why this should be used in any way, because test_target is already a set of predicted classes and doesnt need to be further processed? \n-  Removed the plateu learning rate callback, we can use this in experiment5, but it was not used during exp1 to 4","metadata":{}},{"cell_type":"code","source":"if Train_Advanced_CNN == True:\n    class Config_Advanced_CNN:\n        epochs = 70 #70\n        batch_size =10 # 10\n        exp_name = 4\n        patience = 70 #10\n        initial_learning_rate=0.001 #Initial Learning Rate for ADAM: 0.001\n        optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n        reduce_lr_every_10_epochs = True #Reduce the learning rate each 10 Epochs with the rate defined below\n        lr_reduction_rate = 0.5 # Reduction Rate for reducing each 10 epochs. 0.5 means, that lr is halfed each 10 epochs.\n        Conv1_filter_num = 32\n        Conv1_filter_size = 3\n        filepath_checkpoint = '/kaggle/working/Advanced_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.weights.h5'\n        filepath_accuracy_plot = '/kaggle/working/Advanced_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.accuracy_plot.png'\n        filepath_loss_plot = '/kaggle/working/Advanced_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.loss_plot.png'\n        filepath_classification_report = '/kaggle/working/Advanced_CNN/'+str(Config_Sampling.sample_name)+'/experiment_'+str(exp_name)+'_'+str(Config_Sampling.sample_name)+'.classification_report.txt'\n\n    adv_cnn_model = tf.keras.models.Sequential()\n    adv_cnn_model.add(tf.keras.layers.Conv1D(Config_Advanced_CNN.Conv1_filter_num, Config_Advanced_CNN.Conv1_filter_size, activation='relu', input_shape=(187, 1))) # We add one Conv1D layer to the model\n    adv_cnn_model.add(tf.keras.layers.MaxPooling1D(pool_size=3, strides=2)) # We add one Conv1D layer to the model\n    adv_cnn_model.add(tf.keras.layers.Conv1D(Config_Advanced_CNN.Conv1_filter_num//2, Config_Advanced_CNN.Conv1_filter_size, activation='relu' )) # We add one Conv1D layer to the model\n    adv_cnn_model.add(tf.keras.layers.MaxPooling1D(pool_size=3, strides=2)) # We add one Conv1D layer to the model\n    adv_cnn_model.add(tf.keras.layers.Flatten()) # After  \n    adv_cnn_model.add(tf.keras.layers.Dropout(rate=0.2))\n    adv_cnn_model.add(tf.keras.layers.Dense(120, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n    adv_cnn_model.add(tf.keras.layers.Dense(60, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n    adv_cnn_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n    adv_cnn_model.add(tf.keras.layers.Dense(5, activation='softmax'))\n    \n    def lr_scheduler(epoch, lr):\n        if Config_Advanced_CNN.reduce_lr_every_10_epochs and epoch % 10 == 0: # %10 == 0 means each 10 epochs (only then no \"rest\" after dividing trough 10)\n            return lr * Config_Advanced_CNN.lr_reduction_rate  # reduce the learning rate with the configured rate\n        else:\n            return lr\n\n    # callback for the learning rate reduction schedule\n    lr_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n    \n    # Callback for the model checkpoint\n    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n        filepath=Config_Advanced_CNN.filepath_checkpoint,\n        save_weights_only=True,\n        monitor='val_accuracy',\n        mode='max',\n        save_best_only=True)\n    \n    #Callback for the LR Plateu --> Not used in exp1 to exp4 and therefore commented out, but could be a really good setup for exp5 with all models?\n    \"\"\"LR_plateu_callback = ReduceLROnPlateau(monitor = 'val_acc', min_delta = 0.001, patience = 20,\n                                           factor = 0.5, cooldown = 0, verbose = 1)\"\"\"\n    \n    # Normal early stop callback\n    early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=Config_Advanced_CNN.patience)\n\n    adv_cnn_model.compile(optimizer=Config_Advanced_CNN.optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    adv_cnn_model.summary()\n\n    # Dfs need to be np-array\n    #now we transform Dataframes into numpy arrays\n    train_array = train.to_numpy()\n    test_array = test.to_numpy()\n    print(\"Train Dataframe shape (before reshaping):\", train.shape)\n    print(\"Train Dataframe first 10 rows for comparison:\")\n    #display(train.head(10)) # display(train.head(10)[0]) for first element in first row\n    #display(train.head(10))\n    print(\"train array shape before reshaping:\", train_array.shape)\n    print(\"Train Array before reshaping first 10 rows for comparison:\")\n    #display(train_array[:10]) #display(train_array[:10][0][0]) for first element in first row\n    #display(train_array[:10])\n\n    # Reshape of np arrays for conv1D layer in cnn model\n    train_reshaped = train_array.reshape(train_array.shape[0], train_array.shape[1], 1)\n    test_reshaped = test_array.reshape(test_array.shape[0], test_array.shape[1], 1)\n    print(\"train array shape after reshaping:\", train_reshaped.shape)\n    print(\"Train Array after reshaping first 10 rows for comparison:\")\n    #display(train_reshaped[:10]) #display(train_reshaped[:10][0][0]) for first element of first row\n    \n    # We do not use LR_plateu_callback in exp1 to exp4 and therefore remove it for now (exp5 maybe?)\n    adv_cnn_model_history = adv_cnn_model.fit(train_reshaped, train_target, epochs=Config_Advanced_CNN.epochs, batch_size=Config_Advanced_CNN.batch_size, \n                                      validation_data=(test_reshaped, test_target), \n                                      callbacks=[model_checkpoint_callback, early_stop_callback, lr_scheduler_callback]) #, LR_plateu_callback\n    \n    plt.plot(adv_cnn_model_history.history['accuracy'])\n    plt.plot(adv_cnn_model_history.history['val_accuracy'])\n    plt.legend([\"accuracy\",\"val_accuracy\"])\n    plt.title(f'Accuracy Vs Val_Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.savefig(Config_Advanced_CNN.filepath_accuracy_plot)\n    plt.show()\n    plt.close()\n\n    plt.plot(adv_cnn_model_history.history['loss'])\n    plt.plot(adv_cnn_model_history.history['val_loss'])\n    plt.legend([\"loss\",\"val_loss\"])\n    plt.title('Loss Vs Val_loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.savefig(Config_Advanced_CNN.filepath_loss_plot)\n    plt.show()\n    plt.close()\n\n    predictions = adv_cnn_model.predict(test_reshaped).argmax(axis=1)\n    \n    ## DEBUGGING ERROR ON MAKING CLASSIFICATION REPORT\n    print(\"predictions from adv_cnn_model:\")\n    display(predictions)\n    print(\"\")\n    print(\"test target from adv_cnn_model:\") ###WHY IS TEST TARGET ALSO USED WITH argmax function? is this the same in the other codes? TGest target is NOT used with argtmax in other code.\n    display(test_target)\n    print(\"\")\n    print(f\"Shape of predictions: {predictions.shape}, shape of test target: {test_target.shape} in advanced CNN Model\")\n    \n    report = classification_report(test_target, predictions, digits=4) # test_target used directly without .argmax(axis=1) @Hakan, why did you do this in your notebook?\n    print(report)\n    with open(Config_Advanced_CNN.filepath_classification_report, 'w') as report_file:\n        report_file.write(report)\n\n    print(\"EVERYTHING FINISHED FOR ADVANCED CNN MODEL!\")\n\nelse:\n    print(\"ADVANCED CNN Model is not trained and evaluated\")","metadata":{"execution":{"iopub.status.busy":"2024-03-21T08:21:41.193895Z","iopub.execute_input":"2024-03-21T08:21:41.194485Z","iopub.status.idle":"2024-03-21T08:23:07.748097Z","shell.execute_reply.started":"2024-03-21T08:21:41.194389Z","shell.execute_reply":"2024-03-21T08:23:07.747016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Simple RNN**\nJust the ANN and the first Dense layer is an simple_RNN layer with the same amount of (hidden-)units. The RNN layer should work as follows (doubleckeck before report): The (simple_)RNN layer can be viewed as similar to a dense layer but with the ability to store and update its own internal state based on previous inputs. This allows it to retain information from past time steps and incorporate it into the calculation of the current output. While a dense layer performs a linear transformation on each input independently, the RNN layer's internal state is updated recursively, allowing it to model sequential dependencies.\n#### On the changed input shape of the simple_RNN layer\nThe input shape was changed to (None, 187) to accommodate sequences of variable length, allowing the RNN model to process EKG signals with different sequence lengths efficiently. While the data type remains consistent (EKG signals), this change enables flexibility in handling sequences of varying lengths, which is common in time series data like EKG signals. I.e.: the RNN layer expects this kind of definition for the input shape despite the fact, that we fed it data of the same dimensions all the time.\nJust as for simple_CNN, we have to adjust our input shape to accomodate for the requirements of the specific first layer.","metadata":{}},{"cell_type":"code","source":"if Train_Simple_RNN == True:  \n    class Config_RNN:\n        epochs = 70\n        batch_size = 10\n        exp_name = 1\n        patience = 10  # for early stopping\n        initial_learning_rate = 0.001 #0.001 is default for ADAM\n        optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n        reduce_lr_every_10_epochs = False\n        lr_reduction_rate = 0.5\n        filepath_checkpoint = '/kaggle/working/Simple_RNN/' + str(Config_Sampling.sample_name) + '/experiment_' + str(\n            exp_name) + '_' + str(Config_Sampling.sample_name) + '.weights.h5'\n        filepath_accuracy_plot = '/kaggle/working/Simple_RNN/' + str(Config_Sampling.sample_name) + '/experiment_' + str(\n            exp_name) + '_' + str(Config_Sampling.sample_name) + '.accuracy_plot.png'\n        filepath_loss_plot = '/kaggle/working/Simple_RNN/' + str(Config_Sampling.sample_name) + '/experiment_' + str(\n            exp_name) + '_' + str(Config_Sampling.sample_name) + '.loss_plot.png'\n        filepath_classification_report = '/kaggle/working/Simple_RNN/' + str(Config_Sampling.sample_name) + '/experiment_' + str(\n            exp_name) + '_' + str(Config_Sampling.sample_name) + '.classification_report.txt'\n    \n    # Dfs need to be np-array\n    #now we transform Dataframes into numpy arrays\n    train_array = train.to_numpy()\n    test_array = test.to_numpy()\n    print(\"Train Dataframe shape (before reshaping):\", train.shape)\n    print(\"Train Dataframe first 10 rows for comparison:\")\n    #display(train.head(10)) # display(train.head(10)[0]) for first element in first row\n    #display(train.head(10))\n    print(\"train array shape before reshaping:\", train_array.shape)\n    print(\"Train Array before reshaping first 10 rows for comparison:\")\n    #display(train_array[:10]) #display(train_array[:10][0][0]) for first element in first row\n    #display(train_array[:10])\n\n    # Reshape of np arrays for conv1D layer in RNN model\n    train_reshaped = train_array.reshape(train_array.shape[0], train_array.shape[1], 1)\n    test_reshaped = test_array.reshape(test_array.shape[0], test_array.shape[1], 1)\n    print(\"train array shape after reshaping:\", train_reshaped.shape)\n    print(\"Train Array after reshaping first 10 rows for comparison:\")\n    #display(train_reshaped[:10]) #display(train_reshaped[:10][0][0]) for first element of first row\n    \n    rnn_model = tf.keras.models.Sequential()\n    rnn_model.add(tf.keras.layers.SimpleRNN(60, activation=tf.keras.layers.LeakyReLU(alpha=0.001), input_shape=(train_reshaped.shape[1], train_reshaped.shape[2])))  # Adjusted input shape for RNN\n    rnn_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n    rnn_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n    rnn_model.add(tf.keras.layers.Dense(20, activation=tf.keras.layers.LeakyReLU(alpha=0.001)))\n    rnn_model.add(tf.keras.layers.Dense(5, activation='softmax'))\n\n    def lr_scheduler(epoch, lr):\n        if Config_RNN.reduce_lr_every_10_epochs and epoch % 10 == 0:\n            return lr * Config_RNN.lr_reduction_rate\n        else:\n            return lr\n\n    lr_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n\n    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n        filepath=Config_RNN.filepath_checkpoint,\n        save_weights_only=True,\n        monitor='val_accuracy',\n        mode='max',\n        save_best_only=True)\n    \n    early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=Config_RNN.patience)\n\n    rnn_model.compile(optimizer=Config_RNN.optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    rnn_model.summary()\n    \n    rnn_model_history = rnn_model.fit(train_reshaped, train_target, epochs=Config_RNN.epochs, batch_size=Config_RNN.batch_size,\n                                      validation_data=(test_reshaped, test_target),\n                                      callbacks=[model_checkpoint_callback, early_stop_callback, lr_scheduler_callback])\n\n    plt.plot(rnn_model_history.history['accuracy'])\n    plt.plot(rnn_model_history.history['val_accuracy'])\n    plt.legend([\"accuracy\", \"val_accuracy\"])\n    plt.title('Accuracy Vs Val_Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.savefig(Config_RNN.filepath_accuracy_plot)\n    plt.show()\n    plt.close()\n\n\n    plt.plot(rnn_model_history.history['loss'])\n    plt.plot(rnn_model_history.history['val_loss'])\n    plt.legend([\"loss\", \"val_loss\"])\n    plt.title('Loss Vs Val_loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.savefig(Config_RNN.filepath_loss_plot)\n    plt.show()\n    plt.close()\n\n    predictions = rnn_model.predict(test).argmax(axis=1)\n\n    report = classification_report(test_target, predictions, digits=4)\n    print(report)\n    with open(Config_RNN.filepath_classification_report, 'w') as report_file:\n        report_file.write(report)\n\n    print(\"EVERYTHING FINISHED FOR SIMPLE RNN MODEL!\")\n\nelse:\n    print(\"Simple RNN Model is not trained and evaluated\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T08:23:07.749551Z","iopub.execute_input":"2024-03-21T08:23:07.749850Z","iopub.status.idle":"2024-03-21T08:23:07.773201Z","shell.execute_reply.started":"2024-03-21T08:23:07.749824Z","shell.execute_reply":"2024-03-21T08:23:07.772142Z"},"trusted":true},"execution_count":null,"outputs":[]}]}