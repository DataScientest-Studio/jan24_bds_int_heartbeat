{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96a6a1e1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-08T16:37:17.976321Z",
     "iopub.status.busy": "2024-03-08T16:37:17.976012Z",
     "iopub.status.idle": "2024-03-08T16:37:20.109322Z",
     "shell.execute_reply": "2024-03-08T16:37:20.108455Z"
    },
    "papermill": {
     "duration": 2.143552,
     "end_time": "2024-03-08T16:37:20.111079",
     "exception": false,
     "start_time": "2024-03-08T16:37:17.967527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pickle\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "import xgboost as XGB\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3a84163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/simon/.kaggle/kaggle.json'\n",
      "All Datasets are already available.\n"
     ]
    }
   ],
   "source": [
    "## Test cell for simon: Using Kaggle API to download the datasets indepent of github and its filesize limitations. Storing it in folder located outside of the repo.\n",
    "# If this works, all filepaths have to be adjusted in all notebooks to make use of the downloaded datasets.\n",
    "#RUN THIS CELL ONLY ONCE FOR ALL NOTEBOOKS!\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "#configuring and authentification with kaggle api. This could be configured so that a authentification mask is shown?\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "#Configuring the metadata for the ecg heartbeat data (original username etc)\n",
    "dataset_owner = \"shayanfazeli\"\n",
    "dataset_name = \"heartbeat\"\n",
    "\n",
    "#Configuring a download path that is NOT in the current github repo (so the big files are not pushed and cause an error!) --> Links to filepaths have to be dynamically adjusted\n",
    "download_path = \"../data/KAGGLE_datasets\" #In this case we use the data folder that is in the .gitignore list and therefore not pushed! To keep everything in one local repo.\n",
    "\n",
    "# Download structure: First check if dataset is already downloaded, else download it and store it in download path (should be outside git repo!)\n",
    "dataset_folder = os.path.join(download_path, dataset_name)\n",
    "if not os.path.exists(dataset_folder):\n",
    "    # Case 1: Dataset path is not created --> Create it and download datasets into it\n",
    "    api.dataset_download_files(dataset_owner + \"/\" + dataset_name, path=download_path + \"/\" + dataset_name, unzip=True)\n",
    "    print(\"Datasets are downloaded and unzipped.\")\n",
    "else:\n",
    "    # Case 2: Folder is created, but datasets might be missing\n",
    "    missing_files = [] \n",
    "    for file_name in [\"mitbih_test.csv\", \"mitbih_train.csv\", \"ptbdb_abnormal.csv\", \"ptbdb_normal.csv\"]:  # These are the hardcoded names of the datasets that should be downloaded.\n",
    "        file_path = os.path.join(dataset_folder, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            missing_files.append(file_name)\n",
    "\n",
    "    if missing_files:\n",
    "        # If the list contains missing files, download ALL files and overwrite the old folder.\n",
    "        api.dataset_download_files(dataset_owner + \"/\" + dataset_name, path=download_path + \"/\" + dataset_name, unzip=True, force=True)\n",
    "        print(\"Missing data was donwloaded and unzipped. All Datasets are now available.\")\n",
    "    else:\n",
    "        print(\"All Datasets are already available.\")\n",
    "\n",
    "#Creating new variable that links to the datasets and can be used in the rest of the code.\n",
    "path_to_datasets = download_path + \"/\" + dataset_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06073fe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T16:37:20.125597Z",
     "iopub.status.busy": "2024-03-08T16:37:20.125236Z",
     "iopub.status.idle": "2024-03-08T16:37:20.128675Z",
     "shell.execute_reply": "2024-03-08T16:37:20.128113Z"
    },
    "papermill": {
     "duration": 0.01209,
     "end_time": "2024-03-08T16:37:20.130036",
     "exception": false,
     "start_time": "2024-03-08T16:37:20.117946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#only 4 digits are printed out by numpy calculations.\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71a126b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T16:37:20.144307Z",
     "iopub.status.busy": "2024-03-08T16:37:20.143891Z",
     "iopub.status.idle": "2024-03-08T16:37:28.554329Z",
     "shell.execute_reply": "2024-03-08T16:37:28.553540Z"
    },
    "papermill": {
     "duration": 8.419779,
     "end_time": "2024-03-08T16:37:28.556365",
     "exception": false,
     "start_time": "2024-03-08T16:37:20.136586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell now makes use of the downloadfolder for the datasets. If already available locally, the filepaths can be changed.\n",
    "df_train= pd.read_csv(path_to_datasets + \"/\" + 'mitbih_train.csv', header=None)\n",
    "df_test=pd.read_csv(path_to_datasets + \"/\" +  'mitbih_test.csv',header=None)\n",
    "\n",
    "#split target and value\n",
    "train_target=df_train[187]\n",
    "test_target=df_test[187]\n",
    "train=df_train.drop(187,axis=1)\n",
    "test=df_test.drop(187,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3cbb4eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T16:37:28.570682Z",
     "iopub.status.busy": "2024-03-08T16:37:28.570419Z",
     "iopub.status.idle": "2024-03-08T16:37:28.573932Z",
     "shell.execute_reply": "2024-03-08T16:37:28.573208Z"
    },
    "papermill": {
     "duration": 0.012332,
     "end_time": "2024-03-08T16:37:28.575526",
     "exception": false,
     "start_time": "2024-03-08T16:37:28.563194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Switches for the user to define which sample method is used and which models are run.\n",
    "class Config:\n",
    "    oversample = False #refers to mitbih B_SMOTE\n",
    "    undersample = True #Refers to undersampling with random undersampler\n",
    "    sample_name = \"UNDEFINED_SAMPLE\"\n",
    "\n",
    "Train_SVM =  False #trains the SVM Model without Gridsearch\n",
    "Train_KNN = False #trains the KNN Model without Gridsearch\n",
    "Train_DTC = False #trains the DTC Model without Gridsearch\n",
    "Train_RF = False #trains the RF Model without Gridsearch\n",
    "Train_XGB = False #trains the XGB Model without Gridsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "045ff622",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T16:37:28.590061Z",
     "iopub.status.busy": "2024-03-08T16:37:28.589796Z",
     "iopub.status.idle": "2024-03-08T16:37:28.593402Z",
     "shell.execute_reply": "2024-03-08T16:37:28.592581Z"
    },
    "papermill": {
     "duration": 0.012892,
     "end_time": "2024-03-08T16:37:28.595250",
     "exception": false,
     "start_time": "2024-03-08T16:37:28.582358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "oversampler = SMOTE()\n",
    "undersampler = RandomUnderSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "303ceae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T16:37:28.609288Z",
     "iopub.status.busy": "2024-03-08T16:37:28.609005Z",
     "iopub.status.idle": "2024-03-08T16:37:29.972590Z",
     "shell.execute_reply": "2024-03-08T16:37:29.971888Z"
    },
    "papermill": {
     "duration": 1.372662,
     "end_time": "2024-03-08T16:37:29.974472",
     "exception": false,
     "start_time": "2024-03-08T16:37:28.601810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Name: MITBIH_C_RUS\n"
     ]
    }
   ],
   "source": [
    "#Based on the user settings, Resampling is done and the sample name (i.e. filenames) are modified.\n",
    "if Config.oversample:\n",
    "    train, train_target = oversampler.fit_resample(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n",
    "    Config.sample_name = \"MITBIH_B_SMOTE\"\n",
    "    print(\"Sample Name:\", Config.sample_name)\n",
    "elif Config.undersample:\n",
    "    train, train_target = undersampler.fit_resample(df_train.iloc[:,:-1], df_train.iloc[:,-1])\n",
    "    Config.sample_name = \"MITBIH_C_RUS\"\n",
    "    print(\"Sample Name:\", Config.sample_name)\n",
    "else: \n",
    "    print(\"Using the original mitbih dataset\")\n",
    "    Config.sample_name = \"MITBIH_A_Original\"\n",
    "    print(\"Sample Name:\", Config.sample_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12e9d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to save models and classification report directly after running.\n",
    "def save_model_and_report(model, report, model_filename, report_filename, model_folder=\"../models/ML_Models\", report_folder=\"../reports/figures/ML_Models\"):\n",
    "    # Save the model\n",
    "    model_savepath = os.path.join(model_folder, model_filename)\n",
    "    with open(model_savepath, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"The model was saved as {model_filename} in folder {model_folder}.\")\n",
    "\n",
    "    # Check if model file size is greater than 98MB (Restriction for github only!)\n",
    "    if os.path.getsize(model_savepath) > 98 * 1024 * 1024:  # Check if size is greater than 98MB\n",
    "        print(\"Model file size is too big. Changing save path...\")\n",
    "        model_folder = \"../data/models_too_big_for_git\"\n",
    "        os.makedirs(model_folder, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "        new_model_savepath = os.path.join(model_folder, model_filename)\n",
    "        os.replace(model_savepath, new_model_savepath)  # Move the model to the new location\n",
    "        print(f\"Model moved to {model_folder} due to its size.\")\n",
    "\n",
    "    # Save the classification report\n",
    "    report_savepath = os.path.join(report_folder, report_filename)\n",
    "    with open(report_savepath, \"w\") as f:\n",
    "        f.write(report)\n",
    "    print(f\"The classification report was saved as {report_filename} in folder {report_folder}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da81ef52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T16:37:29.989075Z",
     "iopub.status.busy": "2024-03-08T16:37:29.988820Z",
     "iopub.status.idle": "2024-03-08T16:37:29.994384Z",
     "shell.execute_reply": "2024-03-08T16:37:29.993649Z"
    },
    "papermill": {
     "duration": 0.014706,
     "end_time": "2024-03-08T16:37:29.996128",
     "exception": false,
     "start_time": "2024-03-08T16:37:29.981422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3205, 187)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b9fe67",
   "metadata": {
    "papermill": {
     "duration": 0.006442,
     "end_time": "2024-03-08T16:37:30.009438",
     "exception": false,
     "start_time": "2024-03-08T16:37:30.002996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **SVM**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "140181b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T16:37:30.024672Z",
     "iopub.status.busy": "2024-03-08T16:37:30.024196Z",
     "iopub.status.idle": "2024-03-08T16:37:30.027938Z",
     "shell.execute_reply": "2024-03-08T16:37:30.027182Z"
    },
    "papermill": {
     "duration": 0.013468,
     "end_time": "2024-03-08T16:37:30.029503",
     "exception": false,
     "start_time": "2024-03-08T16:37:30.016035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model withoud gridsearch is not trained\n"
     ]
    }
   ],
   "source": [
    "#Just the code for model creation, fitting and creating the report out of the predictions.\n",
    "if Train_SVM == True:\n",
    "    model = SVC(cache_size=500)\n",
    "    model.fit(train,train_target)\n",
    "    predictions = model.predict(test)\n",
    "    report=classification_report(test_target, predictions, digits=4)\n",
    "    print(report)\n",
    "    #Calling the save_model_and_report function for the SVM Model\n",
    "    model_filename = f\"SVM_Basemodel_no_gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"SVM_Basemodel_no_gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    save_model_and_report(model, report, model_filename, report_filename)\n",
    "else:\n",
    "    print(\"SVM Model withoud gridsearch is not trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66785f33",
   "metadata": {
    "papermill": {
     "duration": 0.006836,
     "end_time": "2024-03-08T17:18:35.863195",
     "exception": false,
     "start_time": "2024-03-08T17:18:35.856359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **KNN**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3784bfce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T17:18:35.878594Z",
     "iopub.status.busy": "2024-03-08T17:18:35.878298Z",
     "iopub.status.idle": "2024-03-08T17:18:35.881911Z",
     "shell.execute_reply": "2024-03-08T17:18:35.881200Z"
    },
    "papermill": {
     "duration": 0.013146,
     "end_time": "2024-03-08T17:18:35.883733",
     "exception": false,
     "start_time": "2024-03-08T17:18:35.870587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Model without gridsearch is not trained.\n"
     ]
    }
   ],
   "source": [
    "if Train_KNN == True:\n",
    "    model = KNN(n_jobs = -1)\n",
    "    model.fit(train,train_target)\n",
    "    predictions = model.predict(test)\n",
    "    report=classification_report(test_target, predictions, digits=4)\n",
    "    print(report)\n",
    "    #Calling the save_model_and_report function for the KNN Model\n",
    "    model_filename = f\"KNN_Basemodel_no_gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"KNN_Basemodel_no_gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    save_model_and_report(model, report, model_filename, report_filename)\n",
    "else:\n",
    "    print(\"KNN Model without gridsearch is not trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278d4c36",
   "metadata": {
    "papermill": {
     "duration": 0.006902,
     "end_time": "2024-03-08T17:30:16.867283",
     "exception": false,
     "start_time": "2024-03-08T17:30:16.860381",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Decision Tree**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ba3453f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T17:30:16.883819Z",
     "iopub.status.busy": "2024-03-08T17:30:16.883518Z",
     "iopub.status.idle": "2024-03-08T17:30:16.886952Z",
     "shell.execute_reply": "2024-03-08T17:30:16.886210Z"
    },
    "papermill": {
     "duration": 0.01312,
     "end_time": "2024-03-08T17:30:16.888499",
     "exception": false,
     "start_time": "2024-03-08T17:30:16.875379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Model without gridsearch is not trained.\n"
     ]
    }
   ],
   "source": [
    "if Train_DTC == True:\n",
    "    model = DTC()\n",
    "    model.fit(train,train_target)\n",
    "    predictions = model.predict(test)\n",
    "    report=classification_report(test_target, predictions, digits=4)\n",
    "    print(report)\n",
    "    #Calling the save_model_and_report function for the Decision Tree Model\n",
    "    model_filename = f\"DTC_Basemodel_no_gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"DTC_Basemodel_no_gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    save_model_and_report(model, report, model_filename, report_filename)\n",
    "else:\n",
    "    print(\"Decision Tree Model without gridsearch is not trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a899a01",
   "metadata": {
    "papermill": {
     "duration": 0.007861,
     "end_time": "2024-03-08T17:34:11.478217",
     "exception": false,
     "start_time": "2024-03-08T17:34:11.470356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Random Forest**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37e1a610",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T17:34:11.495348Z",
     "iopub.status.busy": "2024-03-08T17:34:11.494517Z",
     "iopub.status.idle": "2024-03-08T17:34:11.498346Z",
     "shell.execute_reply": "2024-03-08T17:34:11.497634Z"
    },
    "papermill": {
     "duration": 0.014147,
     "end_time": "2024-03-08T17:34:11.500026",
     "exception": false,
     "start_time": "2024-03-08T17:34:11.485879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model without gridsearch is not trained.\n"
     ]
    }
   ],
   "source": [
    "if Train_RF == True:\n",
    "    model = RFC(n_jobs = -1)\n",
    "    model.fit(train,train_target)\n",
    "    predictions = model.predict(test)\n",
    "    report=classification_report(test_target, predictions, digits=4)\n",
    "    print(report)\n",
    "    #Calling the save_model_and_report function for the Random Forest Model\n",
    "    model_filename = f\"RFC_Basemodel_no_gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"RFC_Basemodel_no_gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    save_model_and_report(model, report, model_filename, report_filename)\n",
    "else:\n",
    "    print(\"Random Forest Model without gridsearch is not trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ef92a",
   "metadata": {
    "papermill": {
     "duration": 0.007353,
     "end_time": "2024-03-08T17:37:53.962258",
     "exception": false,
     "start_time": "2024-03-08T17:37:53.954905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **XGBoost**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a866d4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-08T17:37:53.978948Z",
     "iopub.status.busy": "2024-03-08T17:37:53.978443Z",
     "iopub.status.idle": "2024-03-08T17:37:53.981708Z",
     "shell.execute_reply": "2024-03-08T17:37:53.981176Z"
    },
    "papermill": {
     "duration": 0.013253,
     "end_time": "2024-03-08T17:37:53.983124",
     "exception": false,
     "start_time": "2024-03-08T17:37:53.969871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XBG Model without gridsearch is not trained.\n"
     ]
    }
   ],
   "source": [
    "if Train_XGB == True:\n",
    "    model = XGB.XGBClassifier(objective='binary:logistic')\n",
    "    model.fit(train,train_target)\n",
    "    predictions = model.predict(test)\n",
    "    report=classification_report(test_target, predictions, digits=4)\n",
    "    print(report)\n",
    "    #Calling the save_model_and_report function for the XGB Model\n",
    "    model_filename = f\"XGB_Basemodel_no_gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"XGB_Basemodel_no_gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    save_model_and_report(model, report, model_filename, report_filename)\n",
    "else:\n",
    "    print(\"XBG Model without gridsearch is not trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c9e818",
   "metadata": {},
   "source": [
    "# Gridsearch Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d52f90",
   "metadata": {},
   "source": [
    "We present in the following section the gridsearches, that each team member did. Therefore, the specific codes are presented. If possible,\n",
    "the codes are rewritten to be more compact and fit the overall style of this final notebook. Also the filesaving code lines are added. Other than that, the original code is reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9de0763",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary imports\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import neighbors\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f27d79c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Dataset that is used for the gridsearch: MITBIH_C_RUS\n"
     ]
    }
   ],
   "source": [
    "# Configuration switches / Paramgrids\n",
    "Gridsearch_SVM = True # (Done by Hakan?)\n",
    "Gridsearch_KNN = False # (Done by Alex)\n",
    "Gridsearch_DTC = True # (Done by Alex)\n",
    "Gridsearch_RFC = True # (Done by Simon)\n",
    "Gridsearch_XGB = True # (Done by Hakan)\n",
    "\n",
    "#print the current dataset that is in usage:\n",
    "print(\"Current Dataset that is used for the gridsearch:\", Config.sample_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed41abc",
   "metadata": {},
   "source": [
    "## Gridsearch SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a797309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06b1794e",
   "metadata": {},
   "source": [
    "## Gridsearch KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "56378768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Gridsearch for KNN is performed.\n"
     ]
    }
   ],
   "source": [
    "if Gridsearch_KNN == True:\n",
    "    knn = neighbors.KNeighborsClassifier(n_jobs=-1)\n",
    "    param_knn = {'metric': ['manhattan','minkowski','chebyshev','euclidean'],'n_neighbors': [k for  k in range(1, 7)]}\n",
    "    grid=GridSearchCV(estimator=knn,param_grid=param_knn,verbose=3, cv=5 , scoring='accuracy')\n",
    "    grid.fit(train,train_target)\n",
    "    print(pd.DataFrame(grid.cv_results_)[['params', 'mean_test_score', 'std_test_score']])\n",
    "\n",
    "    print('Best Metric:', grid.best_estimator_.get_params()['metric'])\n",
    "    print('Best K:', grid.best_estimator_.get_params()['n_neighbors'])\n",
    "\n",
    "    y_pred=grid.predict(test)\n",
    "    print(pd.crosstab(test_target,y_pred,rownames=['Class Actual'],colnames=['Class Predict']))\n",
    "    print(classification_report(test_target,y_pred))\n",
    "\n",
    "\n",
    "    print(\"Accuracy Score on Test Set: \", grid.score(test, test_target))\n",
    "    model_filename = f\"KNN_Optimized_Model_with_Gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"KNN_Optimized_Model_with_Gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    #saving the best model and the associated classification report\n",
    "    save_model_and_report(grid.best_estimator_, classification_report(test_target, y_pred), model_filename, report_filename)\n",
    "else:\n",
    "    print(\"No Gridsearch for KNN is performed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e52220",
   "metadata": {},
   "source": [
    "## Gridsearch DTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d6bbea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV 1/5] END .......criterion=gini, max_depth=2;, score=0.540 total time=   0.1s\n",
      "[CV 2/5] END .......criterion=gini, max_depth=2;, score=0.573 total time=   0.1s\n",
      "[CV 3/5] END .......criterion=gini, max_depth=2;, score=0.541 total time=   0.1s\n",
      "[CV 4/5] END .......criterion=gini, max_depth=2;, score=0.574 total time=   0.1s\n",
      "[CV 5/5] END .......criterion=gini, max_depth=2;, score=0.523 total time=   0.1s\n",
      "[CV 1/5] END .......criterion=gini, max_depth=4;, score=0.691 total time=   0.1s\n",
      "[CV 2/5] END .......criterion=gini, max_depth=4;, score=0.760 total time=   0.1s\n",
      "[CV 3/5] END .......criterion=gini, max_depth=4;, score=0.685 total time=   0.1s\n",
      "[CV 4/5] END .......criterion=gini, max_depth=4;, score=0.738 total time=   0.1s\n",
      "[CV 5/5] END .......criterion=gini, max_depth=4;, score=0.719 total time=   0.1s\n",
      "[CV 1/5] END .......criterion=gini, max_depth=6;, score=0.747 total time=   0.2s\n",
      "[CV 2/5] END .......criterion=gini, max_depth=6;, score=0.789 total time=   0.2s\n",
      "[CV 3/5] END .......criterion=gini, max_depth=6;, score=0.754 total time=   0.2s\n",
      "[CV 4/5] END .......criterion=gini, max_depth=6;, score=0.789 total time=   0.2s\n",
      "[CV 5/5] END .......criterion=gini, max_depth=6;, score=0.771 total time=   0.2s\n",
      "[CV 1/5] END .......criterion=gini, max_depth=8;, score=0.757 total time=   0.2s\n",
      "[CV 2/5] END .......criterion=gini, max_depth=8;, score=0.796 total time=   0.2s\n",
      "[CV 3/5] END .......criterion=gini, max_depth=8;, score=0.741 total time=   0.2s\n",
      "[CV 4/5] END .......criterion=gini, max_depth=8;, score=0.816 total time=   0.2s\n",
      "[CV 5/5] END .......criterion=gini, max_depth=8;, score=0.785 total time=   0.2s\n",
      "[CV 1/5] END ......criterion=gini, max_depth=10;, score=0.760 total time=   0.3s\n",
      "[CV 2/5] END ......criterion=gini, max_depth=10;, score=0.799 total time=   0.3s\n",
      "[CV 3/5] END ......criterion=gini, max_depth=10;, score=0.760 total time=   0.3s\n",
      "[CV 4/5] END ......criterion=gini, max_depth=10;, score=0.810 total time=   0.3s\n",
      "[CV 5/5] END ......criterion=gini, max_depth=10;, score=0.791 total time=   0.3s\n",
      "[CV 1/5] END ......criterion=gini, max_depth=12;, score=0.764 total time=   0.3s\n",
      "[CV 2/5] END ......criterion=gini, max_depth=12;, score=0.805 total time=   0.3s\n",
      "[CV 3/5] END ......criterion=gini, max_depth=12;, score=0.755 total time=   0.3s\n",
      "[CV 4/5] END ......criterion=gini, max_depth=12;, score=0.811 total time=   0.3s\n",
      "[CV 5/5] END ......criterion=gini, max_depth=12;, score=0.802 total time=   0.4s\n",
      "[CV 1/5] END ....criterion=entropy, max_depth=2;, score=0.551 total time=   0.1s\n",
      "[CV 2/5] END ....criterion=entropy, max_depth=2;, score=0.587 total time=   0.1s\n",
      "[CV 3/5] END ....criterion=entropy, max_depth=2;, score=0.541 total time=   0.1s\n",
      "[CV 4/5] END ....criterion=entropy, max_depth=2;, score=0.579 total time=   0.1s\n",
      "[CV 5/5] END ....criterion=entropy, max_depth=2;, score=0.591 total time=   0.1s\n",
      "[CV 1/5] END ....criterion=entropy, max_depth=4;, score=0.700 total time=   0.2s\n",
      "[CV 2/5] END ....criterion=entropy, max_depth=4;, score=0.744 total time=   0.2s\n",
      "[CV 3/5] END ....criterion=entropy, max_depth=4;, score=0.699 total time=   0.2s\n",
      "[CV 4/5] END ....criterion=entropy, max_depth=4;, score=0.747 total time=   0.2s\n",
      "[CV 5/5] END ....criterion=entropy, max_depth=4;, score=0.732 total time=   0.2s\n",
      "[CV 1/5] END ....criterion=entropy, max_depth=6;, score=0.760 total time=   0.3s\n",
      "[CV 2/5] END ....criterion=entropy, max_depth=6;, score=0.803 total time=   0.3s\n",
      "[CV 3/5] END ....criterion=entropy, max_depth=6;, score=0.758 total time=   0.3s\n",
      "[CV 4/5] END ....criterion=entropy, max_depth=6;, score=0.807 total time=   0.3s\n",
      "[CV 5/5] END ....criterion=entropy, max_depth=6;, score=0.771 total time=   0.3s\n",
      "[CV 1/5] END ....criterion=entropy, max_depth=8;, score=0.785 total time=   0.4s\n",
      "[CV 2/5] END ....criterion=entropy, max_depth=8;, score=0.789 total time=   0.4s\n",
      "[CV 3/5] END ....criterion=entropy, max_depth=8;, score=0.783 total time=   0.4s\n",
      "[CV 4/5] END ....criterion=entropy, max_depth=8;, score=0.832 total time=   0.4s\n",
      "[CV 5/5] END ....criterion=entropy, max_depth=8;, score=0.791 total time=   0.4s\n",
      "[CV 1/5] END ...criterion=entropy, max_depth=10;, score=0.768 total time=   0.4s\n",
      "[CV 2/5] END ...criterion=entropy, max_depth=10;, score=0.813 total time=   0.4s\n",
      "[CV 3/5] END ...criterion=entropy, max_depth=10;, score=0.771 total time=   0.4s\n",
      "[CV 4/5] END ...criterion=entropy, max_depth=10;, score=0.817 total time=   0.4s\n",
      "[CV 5/5] END ...criterion=entropy, max_depth=10;, score=0.821 total time=   0.4s\n",
      "[CV 1/5] END ...criterion=entropy, max_depth=12;, score=0.764 total time=   0.4s\n",
      "[CV 2/5] END ...criterion=entropy, max_depth=12;, score=0.802 total time=   0.4s\n",
      "[CV 3/5] END ...criterion=entropy, max_depth=12;, score=0.796 total time=   0.4s\n",
      "[CV 4/5] END ...criterion=entropy, max_depth=12;, score=0.822 total time=   0.4s\n",
      "[CV 5/5] END ...criterion=entropy, max_depth=12;, score=0.805 total time=   0.4s\n",
      "Best Metric: entropy\n",
      "Best Depth: 10\n",
      "Class Predict    0.0   1.0   2.0   3.0   4.0\n",
      "Class Actual                                \n",
      "0.0            12919  2373  1201  1105   520\n",
      "1.0               98   408    30    11     9\n",
      "2.0               97    46  1147   105    53\n",
      "3.0               11     2    15   133     1\n",
      "4.0               42     8    37    12  1509\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.71      0.83     18118\n",
      "         1.0       0.14      0.73      0.24       556\n",
      "         2.0       0.47      0.79      0.59      1448\n",
      "         3.0       0.10      0.82      0.17       162\n",
      "         4.0       0.72      0.94      0.82      1608\n",
      "\n",
      "    accuracy                           0.74     21892\n",
      "   macro avg       0.48      0.80      0.53     21892\n",
      "weighted avg       0.90      0.74      0.79     21892\n",
      "\n",
      "Accuracy Score on Test Set:  0.7361593276082588\n",
      "The model was saved as DTC_Optimized_Model_with_Gridsearch_MITBIH_C_RUS.pkl in folder ../models/ML_Models.\n",
      "The classification report was saved as DTC_Optimized_Model_with_Gridsearch_MITBIH_C_RUS_classification_report.txt in folder ../reports/figures/ML_Models.\n"
     ]
    }
   ],
   "source": [
    "if Gridsearch_DTC == True:\n",
    "    dt=DecisionTreeClassifier()\n",
    "    param_dt={'criterion': ['gini', 'entropy'],   'max_depth': [2,4,6,8,10,12]}\n",
    "    grid=GridSearchCV(estimator=dt,param_grid=param_dt, verbose=3, cv=5 , scoring='accuracy')\n",
    "    grid.fit(train, train_target)\n",
    "    pd.DataFrame(grid.cv_results_)[['params', 'mean_test_score', 'std_test_score']]\n",
    "\n",
    "    print('Best Metric:', grid.best_estimator_.get_params()['criterion'])\n",
    "    print('Best Depth:', grid.best_estimator_.get_params()['max_depth'])\n",
    "\n",
    "    y_pred=grid.predict(test)\n",
    "    print(pd.crosstab(test_target,y_pred,rownames=['Class Actual'],colnames=['Class Predict']))\n",
    "    print(classification_report(test_target,y_pred))\n",
    "\n",
    "    print(\"Accuracy Score on Test Set: \", grid.score(test, test_target))\n",
    "    model_filename = f\"DTC_Optimized_Model_with_Gridsearch_{Config.sample_name}.pkl\"\n",
    "    report_filename = f\"DTC_Optimized_Model_with_Gridsearch_{Config.sample_name}_classification_report.txt\"\n",
    "    #saving the best model and the associated classification report\n",
    "    save_model_and_report(grid.best_estimator_, classification_report(test_target, y_pred), model_filename, report_filename)\n",
    "else:\n",
    "    print(\"No Gridsearch for DTC is performed.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 29414,
     "sourceId": 37484,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3755.277893,
   "end_time": "2024-03-08T17:39:50.297430",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-08T16:37:15.019537",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
